# Classification
Everything about classification


---------------------------------------------

**Repository summary**

1.  **Intro** üß≥

2.  **Tech Stack** ü§ñ

3.  **Features** ü§≥üèΩ

4.  **Process** üë£

5.  **Learning** üí°

6.  **Improvement** üî©

7.  **Running the Project** ‚öôÔ∏è

8 .  **More** üôåüèΩ




---------------------------------------------

# :computer: Everything about Classification  :computer:

---------------------------------------------

# 1. Let's talk about Classification.

Classification is everywhere: even when we don‚Äôt notice it.  
Every day, dozens of systems quietly classify information around us:  
Netflix decides what you might like next.  
Gmail separates spam from important messages.  
Spotify predicts the next song that fits your mood.  
A bank model decides whether to approve your credit card.  

These are not coincidences. They are **classification models** ‚Äî algorithms that learn from data to assign categories, make predictions, and help humans make faster, more consistent decisions.

In the modern world, classification is the foundation of intelligent applications.  
It powers everything from voice assistants and medical diagnosis systems to fraud detection, recommendation engines, and autonomous vehicles.  

This repository is dedicated to understanding these models ‚Äî from the simplest logistic regression to the most advanced neural networks ‚Äî explaining not only how they work, but also when and why to use them.

Let‚Äôs explore how machines learn to **draw boundaries, make decisions, and see patterns in data**.


 # I. Introduction: what is Classification?

Classification stands as one of the pillars of modern machine learning. At its essence, it is the act of making a decision ‚Äî determining to which category or class a given observation belongs. When an email service filters a message as spam, when a hospital system predicts whether a patient is at high or low risk, or when a smartphone recognizes a face to unlock the screen, it is performing a classification task. In each of these situations, a model transforms patterns in the data into meaningful, structured outcomes that help automate everyday decisions.

At its core, classification seeks to answer a timeless question: *Given what I have learned from the past, how should I categorize what I see now?*  
This question sits at the heart of predictive intelligence, bridging the gap between observation and action.

### 1. Definition and Essence.

In the language of data science, classification is a supervised learning task in which a model learns from labeled data how to assign new observations to predefined categories. During training, the algorithm is exposed to examples that contain both the input features ‚Äî the measurable characteristics of each case ‚Äî and their corresponding labels ‚Äî the known outcomes. Over time, it captures patterns that describe the relationship between inputs and outputs. When confronted with unseen data, it predicts the most likely class by applying what it has learned.

Unlike regression, which estimates continuous quantities, classification produces discrete outcomes. These outcomes represent states, events, or entities that are distinct and mutually exclusive. This fundamental difference shapes every aspect of modeling: the type of loss functions we use, the interpretation of accuracy, and the criteria we employ to evaluate success. While regression answers ‚Äúhow much,‚Äù classification answers ‚Äúwhich one.‚Äù

### 2. Domains and Data Types.

The power of classification lies in its versatility. It extends far beyond spreadsheets or tabular records to virtually every form of information that can be captured and quantified. In text, it identifies whether a tweet expresses positive or negative sentiment, or whether an email belongs in the spam folder. In images, it distinguishes between cats, dogs, and countless objects of daily life, supporting everything from medical diagnostics to autonomous vehicles. In audio, it allows a voice assistant to recognize who is speaking and what is being said. Even in industrial systems, classification monitors sensor data to detect failures or anomalies before they occur.

If data can be represented numerically ‚Äî whether as words, pixels, or waveforms ‚Äî it can be classified. This universality explains why classification has become one of the cornerstones of artificial intelligence. It is the common language that connects different data modalities under a single analytical goal: the pursuit of structured understanding.

### 3. Types of Classification problems.

Not all classification problems are created equal. Some are simple, others profoundly complex. The most basic form, binary classification, distinguishes between two possible outcomes: an email is either spam or not spam; a transaction is fraudulent or legitimate. Multiclass problems expand the scope by allowing more than two categories, as when a model identifies the species of a flower or the digit drawn on a touchscreen. More advanced yet are multilabel problems, in which a single instance can belong to several categories simultaneously ‚Äî a news article that fits both ‚Äúpolitics‚Äù and ‚Äúeconomy,‚Äù for example. At the highest level of abstraction lies hierarchical classification, where classes follow a structured taxonomy, much like the biological hierarchy that organizes species into genera and families.

Understanding which kind of problem one faces is not merely academic. It defines how the data must be prepared, how the algorithm must be chosen, and how the results must be measured. The type of classification shapes the entire analytical journey.

### 4. The spectrum of complexity.

The world of classification algorithms forms a continuum that ranges from elegant simplicity to profound complexity. At one end are interpretable, mathematically transparent models such as Logistic Regression or Linear Discriminant Analysis. These methods rely on linear relationships and serve as powerful baselines, offering insight into how features contribute to decisions. In the middle of the spectrum appear models like Decision Trees, Random Forests, and Support Vector Machines, which are capable of capturing nonlinear interactions and subtle dependencies among variables. At the farthest end, deep neural networks extend classification into unstructured domains ‚Äî images, text, sound ‚Äî by learning multiple layers of abstract representation.

This gradual increase in complexity reflects the evolution of classification itself. It has grown from the study of simple linear boundaries into a vast family of methods capable of modeling the intricacies of human perception and reasoning. The same mathematical principle that once separated points on a plane now enables machines to distinguish a pedestrian from a shadow, or a heartbeat from an anomaly.

### 5. Why It Matters?

To understand classification is to understand the foundation of intelligent systems. Almost every predictive model that informs a decision ‚Äî whether it approves a loan, recommends a movie, or detects a security threat ‚Äî depends on some form of classification. Knowing how these models work allows data scientists not only to build accurate predictors but also to interpret them responsibly.

Classification teaches us to separate signal from noise, to recognize uncertainty as part of knowledge, and to express probability as a measure of belief rather than certainty. It enables practitioners to calibrate confidence, quantify risk, and explain outcomes in a language that both machines and humans can understand. Its importance extends beyond technical accuracy: it brings rigor and transparency to the very process of decision-making in the digital age.

In disciplines such as healthcare or finance, this understanding becomes ethical as well as practical. A misclassified tumor or a misjudged credit risk can have real consequences. For that reason, mastering classification means mastering both the science and the responsibility behind automated decisions.

### 6. From models to meaning: the human connection.

Every classification model is, in a sense, a reflection of human reasoning. When a doctor recognizes a disease from symptoms, when a teacher evaluates an essay, or when a driver decides whether to stop or continue, they are performing a classification task ‚Äî grouping observations into categories based on experience and context. Machine learning formalizes this intuition through algorithms and data, allowing it to scale beyond individual judgment to millions of decisions per second. Yet, behind every model, the essence remains human: the desire to understand, to predict, and to act consistently upon patterns in the world.

----

To study classification deeply, one must go beyond algorithms and consider the ingredients that make them reliable. Before building models, we must understand the forces that shape their behavior ‚Äî how class imbalance distorts performance, how probabilities lose calibration, and how the choice of thresholds and metrics defines what ‚Äúsuccess‚Äù truly means. These foundational ideas are the **core components of classification analysis**, and they set the stage for every method that follows.

----

# II. üß© Core Components of classication analysis. üß©

Classification becomes meaningful when we treat it as a complete process, not just a model choice. A robust analysis begins with a precise understanding of the decision we want to support, continues with careful treatment of data and representation, and matures through thoughtful training, evaluation, interpretation, and monitoring. This section describes that end-to-end path. The goal is simple and ambitious at the same time: build models that perform well, explain themselves clearly, and withstand real-world conditions.

## 1. Understanding the problem and the data.

A solid project starts when the team defines the question with care. The analyst states the decision that the model will support, clarifies who will use the prediction, and documents the consequence of being wrong. A hospital wants to flag patients at high risk, a bank wants to screen transactions for fraud, and a public agency wants to prioritize inspections. Each case carries a different cost for false positives and false negatives, and each cost changes how we should evaluate success.

The analyst also identifies the type of classification at stake. A binary task separates two outcomes, a multiclass task assigns one among many labels, and a multilabel task allows several labels at once. A hierarchical task arranges labels in levels and requires consistent decisions across the hierarchy. The team then audits the sources of data, including tabular records, documents, images, audio, or sensor streams, and writes a short data card that lists provenance, refresh cycles, and known limitations. This early discipline prevents confusion later and aligns the modeling effort with the decisions that matter.

## 2. Data preparation and cleaning.

Clean data creates reliable learning. The analyst handles missing values in a way that respects the data generating process, detects outliers that break assumptions, and fixes inconsistencies that arise when systems merge. Categorical variables receive encodings that preserve information content, numerical variables receive scaling when algorithms require it, and time fields receive careful parsing to avoid hidden leakage. The team draws a clear boundary between training data and future data, because leakage appears the moment information from the future slips into the past. The final step splits the dataset into training, validation, and test partitions, or establishes cross-validation folds that respect grouping or time order. This structure keeps the estimate of generalization honest.

## 3. Feature engineering and representation.

Features shape what a model can learn. The analyst begins with a minimal set that captures the essence of the domain and then improves representation with transformations and interactions. A credit model benefits from ratios that express behavior across time, a medical model benefits from trends rather than single snapshots, and a text model benefits from embeddings that preserve semantics. The team removes redundant or perfectly collinear features when they add noise, and applies dimensionality reduction when the number of variables grows faster than the number of examples. The choice of representation influences linear and nonlinear models in different ways. A linear model rewards informative transformations, while a tree ensemble tolerates raw forms but still benefits from well-designed features. Good representation reduces variance, clarifies signal, and lowers the burden on the learning algorithm.

## 4. Model selection and training.

Model selection follows from problem structure, data regime, and interpretability needs. A team chooses logistic regression or linear discriminant analysis when the relationship appears close to linear and when stakeholders demand transparent coefficients. A team chooses decision trees, random forests, or gradient boosted trees when relationships appear nonlinear and tabular data dominates. A team chooses support vector machines when margins matter and the kernel trick unlocks structure. A team chooses neural networks when images, audio, or language require hierarchical representation.

Training becomes a controlled experiment. The analyst defines an objective function that matches the task, tunes hyperparameters with a method that balances thoroughness and compute budget, and applies regularization to prevent overfitting. The team handles class imbalance with class weights or resampling and reserves synthetic methods for cases where the signal does not reach the minority class without help. The training loop tracks convergence, records seeds for reproducibility, and logs configurations for future audits. A small set of baselines anchors the work, because a strong baseline protects the team from chasing noise.

## 5. Validation and evaluation.

Evaluation turns predictions into judgment. The analyst selects metrics that match the decision, not the dataset convenience. A rare event demands precision and recall, while a balanced screening task tolerates accuracy as a summary. Receiver operating characteristic curves describe ranking power across thresholds, while precision‚Äìrecall curves reveal performance where positive cases are scarce. The team reads confusion matrices per class and studies error types with the same attention a clinician gives to symptoms. Threshold analysis appears as a natural step, because business value rarely aligns with a fixed probability of 0.5. Calibration closes the loop by aligning predicted probabilities with observed frequencies. A calibrated model gives a reliable measure of confidence, which becomes essential when humans use scores to plan actions.

Cross-validation estimates generalization when data are limited, and temporal splits protect time-dependent structure when events evolve. Robustness checks matter as much as headline scores. The team perturbs inputs, repeats runs with different random seeds, and measures variance across folds. The analyst prefers a model that performs slightly worse on average but more consistently across time and subgroups, because stability sustains trust.

## 6. Interpretation and explainability.

A model serves people when it explains itself. The analyst studies which features drive predictions and which interactions change decisions at the margin. Global importance summarizes the broad picture, while local explanations illuminate individual cases. Tools such as permutation importance, SHAP values, and counterfactual analysis provide complementary views, yet none replaces domain sense. The team validates explanations with subject matter experts and documents how limitations might affect use. The analyst quantifies uncertainty, communicates it with clarity, and avoids false certainty, because confidence without calibration misleads more than it helps. Interpretation does not exist to decorate reports. It exists to support responsible action.

## 7. Deployment and monitoring.

A model becomes real when it leaves the notebook. The team exports artifacts in a portable format, wraps them in an interface that other systems can call, and writes a small contract that states input schema, output schema, and expected latencies. Monitoring begins on day one. The analyst tracks predictive performance and watches for data drift and concept drift, because the world changes slowly and then all at once. When drift appears, the team investigates whether feature distributions moved, whether labels shifted, or whether behavior changed in ways the original training could not anticipate. A feedback loop allows the model to learn again, but only after the team evaluates risks and documents the change.

## 8. Ethical and responsible AI considerations.

A classification system touches people, therefore it carries responsibility. The analyst inspects data for biases that hurt protected groups, measures fairness with metrics that reflect the institution‚Äôs values, and records trade-offs that appear when fairness constraints meet accuracy goals. Transparency matters, because users deserve to understand how a system reaches conclusions that affect them. Accountability matters, because a team should be able to trace a decision back to data, code, and configuration. Responsible practice does not slow progress. Responsible practice makes progress sustainable.

## 9. From process to practice.

The classification pipeline behaves like a cycle rather than a line. The analyst learns from errors, revisits data preparation, simplifies features, and resets baselines when the signal does not support complexity. Documentation becomes a habit rather than a chore, because future readers will depend on the record to reproduce results and extend the work. The team treats experiments as evidence, not as decoration, and accepts that small improvements rarely justify fragile systems. Over time, this discipline produces models that travel well from lab to production and remain useful after the initial excitement fades.

---

Before we introduce individual algorithms, we should master the forces that quietly govern their behavior. Class imbalance changes what success looks like, calibration reshapes the meaning of probability, and threshold selection turns ranked scores into real decisions. Metrics complete the picture by defining value in a way the project can defend. These transversal topics represent the grammar of classification. Once we speak that grammar with confidence, every method we study will become clearer, more comparable, and easier to deploy with integrity.

---

# III.  Cross-cutting Topics (Core Analytical Foundations).

Classification is not only about choosing an algorithm.
Before any model can make a fair, reliable, and interpretable decision, we must understand the invisible forces that govern its behavior. These forces‚Äîclass imbalance, probability calibration, threshold selection, and evaluation metrics‚Äîshape how models learn, predict, and fail. They are called cross-cutting topics because they apply to every classifier, regardless of its mathematical form.
Ignoring them leads to models that perform well on paper but fail in reality; mastering them turns simple models into trustworthy tools.

## 1. Introduction: Why Cross-cutting Topics Matter

Every classification model learns patterns, but those patterns emerge inside a context defined by the data and by the decisions we expect to make. These cross-cutting topics form that context. They determine how models interpret uncertainty, how predictions translate into actions, and how performance should be measured in a meaningful way.

For example, a bank that predicts default risk does not simply want to maximize accuracy‚Äîit wants to minimize losses while treating clients fairly. A hospital that predicts disease risk must choose a threshold that saves lives without overwhelming resources. In both cases, the success of the model depends less on the algorithm itself and more on how we handle imbalance, probabilities, and evaluation.

Understanding these foundations allows data scientists to move beyond superficial metrics. It builds the capacity to reason statistically, ethically, and operationally at the same time.

## 2. Class Imbalance

Most real-world classification problems are unbalanced. Fraud cases are rare compared to normal transactions, diseases occur in a small fraction of patients, and positive reviews may outnumber negative ones. When the majority class dominates, models learn to favor it, achieving high accuracy while failing to detect the minority class‚Äîthe one that often matters most.

This imbalance distorts the decision boundary. A model that predicts ‚Äúno fraud‚Äù 99 % of the time might still reach 99 % accuracy, yet it becomes useless when the goal is to detect fraud. The key lies in restoring balance, not by forcing equal counts, but by giving the minority class the attention it deserves.

There are three complementary strategies.
First, algorithmic adjustments, such as using class weights or cost-sensitive learning, tell the model that certain errors are more expensive than others.
Second, data-level techniques, like under-sampling the majority or over-sampling the minority, provide more balanced training examples. Synthetic methods like SMOTE (Synthetic Minority Over-sampling Technique) and ADASYN create plausible new observations to enrich scarce categories.
Third, evaluation-level corrections, such as preferring precision‚Äìrecall curves or F1-scores instead of plain accuracy, ensure that metrics reflect real performance.

The right choice depends on context. A hospital might tolerate more false alarms if it means catching every positive case. A bank might accept a few missed frauds if it avoids wrongly flagging honest clients. The lesson is simple: imbalance changes what success looks like, and every responsible analyst must recognize that early.

## 3. Probability Calibration

A well-calibrated model not only predicts correctly but also tells the truth about its confidence.
If a model says, ‚Äúthere is a 70 % chance of rain,‚Äù it should rain on about 7 out of 10 such days. This alignment between predicted probabilities and observed outcomes defines calibration.

Many algorithms, however, produce distorted probabilities. Decision trees and boosting methods tend to output overconfident scores because they optimize separation rather than probability estimation. Neural networks can also become miscalibrated when trained with aggressive regularization or imbalanced data.

To fix this, analysts apply post-processing calibration techniques.
The most common are Platt scaling, which fits a logistic regression on the model‚Äôs outputs; Isotonic regression, which uses nonparametric fitting for greater flexibility; and Temperature scaling, a simple yet effective approach for neural networks.

To evaluate calibration, we can use the Brier score, which measures the mean squared difference between predicted and actual outcomes, or reliability diagrams, which visualize how predicted probabilities align with observed frequencies.

In domains like credit scoring or medical diagnosis, calibration equals trust. A miscalibrated model might assign false confidence to risky loans or overlook uncertain diagnoses. Correcting that bias ensures that probability means probability, not just a score.

## 4. Threshold Selection

Every classifier ultimately faces a single, deceptively simple question:
At what point should we say ‚Äúyes‚Äù?

The conventional threshold of 0.5 is a convenience, not a law of nature.
Different problems require different cutoffs. In a medical triage system, setting a lower threshold may catch more potential patients at risk but also generate more false alarms. In a credit approval system, a higher threshold may reduce default rates but deny opportunities to worthy clients.

Threshold selection transforms continuous model outputs into categorical decisions. It defines the balance between precision (how often we are right when we say ‚Äúyes‚Äù) and recall (how often we find all true positives). Choosing a threshold thus becomes a moral, economic, and operational decision, not just a statistical one.

Analysts use several strategies to find optimal thresholds. The Youden J statistic maximizes the distance between the true positive rate and the false positive rate. The F1-score seeks a harmonic balance between precision and recall. Cost-based thresholds incorporate explicit penalties for different types of errors.
Each method reveals a different aspect of the trade-off.

By treating threshold selection as an intentional act rather than an afterthought, we make models actionable and context-aware. A well-chosen threshold transforms a model from a calculator into a decision aid.

## 5. Evaluation Metrics

Metrics define what ‚Äúgood‚Äù means in machine learning.
A model is only as good as the measure we choose to judge it. Selecting the wrong metric can reward the wrong behavior.

For balanced problems, accuracy might suffice, but in unbalanced scenarios, it hides the truth. Analysts instead rely on a suite of metrics that capture performance from multiple angles.
Ranking metrics, such as ROC-AUC (Area Under the Receiver Operating Characteristic Curve) and PR-AUC (Area Under the Precision‚ÄìRecall Curve), evaluate how well the model ranks positives over negatives.
Class-level metrics, such as precision, recall, and the F1-score, focus on correctness within each class. MCC (Matthews Correlation Coefficient) summarizes overall consistency between predictions and reality.
Probabilistic metrics, such as Log-loss and the Brier score, evaluate not only correctness but also confidence.

These metrics complement each other. A model with high recall but low precision may be useful in emergency screening; a model with high precision but moderate recall may suit fraud prevention. Analysts must also consider calibration curves, confusion matrices, and threshold analysis to fully understand behavior.

Metrics are not neutral‚Äîthey encode priorities and ethics. Choosing F1 over accuracy, or PR-AUC over ROC-AUC, reflects a statement of values about what outcomes matter most. Understanding this dimension turns evaluation into a deliberate, ethical act.

## 6. Integrative Perspective

The beauty and complexity of classification lie in how these elements interact.
Imbalance changes the meaning of metrics. Calibration reshapes probability interpretation. Thresholds shift the trade-off between risk and reward. Together, they define the operational identity of a model.

Treating these concepts in isolation often leads to contradictions.
An analyst may fix imbalance but forget to recalibrate probabilities; another may adjust thresholds without reconsidering which metric to optimize. The integrative view recognizes that each adjustment influences the others.

Responsible practice, therefore, means iteration. The analyst builds, measures, adjusts, and re-evaluates until the system‚Äôs outputs align with both empirical accuracy and practical intent. A good model fits not only data but also purpose.

## 7. Implementing Cross-cutting Topics in Practice

After understanding the theory, the next step is applying it through code.
Modern programming languages provide robust ecosystems to operationalize these analytical foundations.

Python leads the field with libraries such as scikit-learn, which implements sampling strategies, calibration methods, and metric evaluation; imbalanced-learn, which adds advanced resampling algorithms like SMOTE and ADASYN; and visualization tools like matplotlib, seaborn, and plotly for diagnostic plots and calibration curves.
Other libraries, including xgboost, lightgbm, and catboost, incorporate internal mechanisms to manage imbalance and calibration directly during training.

In R, frameworks such as caret, mlr3, and tidymodels provide parallel functionality with a clear statistical foundation. They support cross-validation, weighting schemes, and interpretable evaluation pipelines, often favored by analysts in academia and healthcare.

These languages promote reproducibility and open experimentation. They bridge the analytical insights discussed here with the technical implementations that bring them to life, ensuring that theory becomes measurable, testable, and transparent.


---- 

Having explored the analytical foundations that influence every model, we are now ready to study the models themselves.
The next section presents the Taxonomy of Classification Models‚Äîa structured map of the main families of classifiers, from the simplicity of logistic regression to the expressive depth of neural networks.

With the knowledge of imbalance, calibration, thresholds, and metrics, we can now compare models not only by accuracy but by their fit to context, interpretability, and fairness.
This transition marks the moment when understanding becomes craftsmanship, and where classification evolves from an abstract concept into a concrete, reproducible system.

----

# IV. Taxonomy of Classification Models.

Classification is a vast landscape, and each model represents a different way of seeing the world. To classify is not merely to compute; it is to choose a philosophy of learning ‚Äî a way to interpret data, uncertainty, and structure.
This taxonomy serves as a map through that landscape, tracing how ideas have evolved from simple linear equations to deep architectures capable of discovering meaning in text, vision, and sound.
Here, we focus on understanding the logic behind each family of models, not the formulas or code. We will explore how they were born, what they solve, and why they continue to matter.

## 1. Introduction to Model Taxonomy

Every classification model answers a slightly different question.
Linear and probabilistic models ask, ‚ÄúCan I draw a line that separates these points?‚Äù Margin-based models ask, ‚ÄúHow far apart can I push them?‚Äù Instance-based models respond, ‚ÄúWhom do I resemble most?‚Äù Tree-based models inquire, ‚ÄúWhat sequence of questions best splits this space?‚Äù Ensembles reply, ‚ÄúWhat if we let many weak opinions vote?‚Äù And neural networks whisper, ‚ÄúCan the machine learn its own representation?‚Äù

This is the logic of progression ‚Äî from the explicit to the emergent, from geometry to hierarchy.
Linear models begin with human-defined relationships; deep networks end with patterns no human could describe but that still mirror intuition.
Understanding this taxonomy is like standing on a mountain and watching the valleys of Machine Learning unfold below.
We begin where everything started ‚Äî with linearity and probability.

## 2. Linear and Probabilistic Models

At the dawn of machine learning stood linearity ‚Äî the belief that the world could be described by a weighted sum of inputs. Logistic Regression, Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Naive Bayes emerged from statistical thinking long before the term ‚Äúmachine learning‚Äù was coined.

Their philosophy is simple: assume structure, estimate parameters, and predict categories based on explicit mathematical relationships. Logistic regression models the log-odds of belonging to a class, while LDA and QDA model the underlying distributions of each class and the boundary where they meet. Naive Bayes, though based on a strong independence assumption, exploits the same probabilistic foundation.

These models remain powerful because they are transparent and stable. A logistic regression coefficient can be explained to a policymaker; an LDA score can be interpreted by a doctor. They thrive when data are limited and interpretability matters more than raw performance.

Yet their beauty is also their boundary. When relationships are nonlinear or highly interactive, linear separability collapses. That limitation inspired a new generation of models that replaced probability with geometry ‚Äî the margin-based family.

## 3. Margin-based Models

Margin-based models redefined learning by geometry rather than probability.
The Perceptron, developed in the 1950s, sought a linear boundary that classified all examples correctly. It worked ‚Äî until data became messy.
Decades later, Support Vector Machines (SVMs) resurrected the idea with mathematical rigor, maximizing the margin between classes and allowing for imperfect separations through soft constraints.

Their central idea is elegant: instead of fitting many points closely, find the boundary that lies farthest from both classes ‚Äî the safest possible separator. Through kernel functions, SVMs project data into higher-dimensional spaces where complex patterns become linearly separable.

Margin-based models marked the shift from explicit distributional assumptions to optimization and geometry. They balance power and parsimony, generalizing well even on small datasets with many features, such as text classification or genomics.
However, interpretability and scalability suffer as kernels grow complex, making them harder to tune and understand.

Their geometric focus paved the way for the next question: if the best boundary depends on proximity, what if we skip modeling altogether and let neighbors decide directly? Thus emerged instance-based learning.

## 4. Instance-based Models

Instance-based models classify not by abstraction but by resemblance.
The k-Nearest Neighbors (kNN) algorithm embodies the intuition that similarity implies identity. To predict a new observation, kNN looks at the closest existing examples in feature space and assigns the most common label among them.

This approach is lazy ‚Äî it performs almost no training, deferring all computation to prediction time.
Its simplicity is disarming and its logic profoundly human. We often reason this way ourselves: a new case reminds us of a previous one, and we draw analogies.

Instance-based models excel when decision boundaries are irregular and data are dense. They perform well in small, low-dimensional spaces where local structure matters. Yet they struggle as data scale. Storing and comparing every observation becomes expensive; small changes in distance metrics can flip outcomes dramatically.

Their strengths and weaknesses led researchers to seek hierarchical decisions ‚Äî a way to learn compact, interpretable structures that could still model nonlinearity. The answer arrived through trees.

##  5. Tree-based Models

Decision Trees changed how we think about classification.
Instead of fitting equations or comparing points, trees ask questions:
‚ÄúIs income greater than 50,000?‚Äù ‚ÄúIs the patient‚Äôs age above 45?‚Äù
At each node, a split divides the data into purer groups, recursively creating a tree of logical decisions that ends with predicted classes.

Models such as CART (Classification and Regression Trees) and ID3 introduced this principle of recursive partitioning.
The result is intuitive: one can read a tree top-down like a decision manual. Trees handle numeric and categorical features, capture interactions, and require minimal preprocessing.

Their main drawback lies in instability. A small change in data can yield an entirely different tree. They also tend to overfit, learning patterns too specific to the training sample. But trees reintroduced interpretability at a time when models were becoming increasingly opaque.
They set the stage for one of the most transformative ideas in machine learning ‚Äî combining many weak trees into one strong forest.

## 6. Ensemble Models

The idea of ensembles revolutionized predictive modeling: the collective opinion of many weak learners is often stronger than the judgment of one.
Methods like Bagging (bootstrap aggregating) and Random Forests reduce variance by averaging over many diverse trees, while Boosting methods such as AdaBoost, XGBoost, LightGBM, and CatBoost sequentially correct errors made by previous models.

Ensembles embody the principle of collective intelligence. Each individual tree may be inaccurate, but together they form a robust, flexible predictor.
These models achieve state-of-the-art performance across domains, from credit risk to recommendation systems and beyond.

However, their strength introduces opacity. Hundreds of trees make it difficult to explain why a specific decision was made. Moreover, ensembles demand more computational resources and tuning.
Despite these challenges, they remain the workhorses of modern ML ‚Äî balancing accuracy, generalization, and practical usability.

As data grew in size and complexity, however, even ensembles struggled to capture abstract relationships in images, audio, and text. The next frontier required models that could learn representations themselves ‚Äî leading to the era of neural networks.

## 7. Neural Networks for Classification

Neural networks transformed machine learning by introducing representation learning: the ability to discover features automatically through layers of computation.
Early Multilayer Perceptrons (MLPs) extended the logic of linear models with nonlinear activations, capturing complex relationships in tabular data.
Convolutional Neural Networks (CNNs) revolutionized computer vision by learning spatial hierarchies of features: edges, shapes, objects.
Recurrent Neural Networks (RNNs) and later LSTMs and Transformers enabled sequence understanding in language, speech, and time series.

Their conceptual shift was profound: rather than designing features manually, analysts now design architectures that learn features. The model becomes both a learner and a feature engineer.
Neural networks can approximate virtually any function, achieving extraordinary results in domains once thought unreachable by algorithms.

But power comes with cost. These systems require vast data, hardware acceleration, and careful tuning. Interpretability becomes difficult ‚Äî decisions arise from thousands of parameters interacting in opaque ways.
Yet their capacity for abstraction has expanded what classification can mean: not just labeling, but understanding.

This complexity closes the taxonomy loop ‚Äî from the transparent logic of equations to the emergent reasoning of layered representations.

## 8. Comparative Reflection: Complexity vs Interpretability

The evolution of classification models is not a ladder of superiority but a landscape of trade-offs.
Linear models are transparent and grounded, ideal when interpretability and causality matter.
Margin-based and instance-based models add geometric sophistication at the expense of simplicity.
Trees and ensembles democratize nonlinearity, achieving strong predictive power with diminishing transparency.
Neural networks push performance further but challenge our ability to explain and trust.

Maturity in data science means choosing the simplest model that solves the problem while respecting ethical and practical boundaries.
A logistic regression that performs nearly as well as a deep network is often the better choice when clarity, speed, or accountability matter.
Each family contributes a different philosophy of learning; progress lies not in abandoning the old but in understanding when to use each wisely.

---- 

In the next section, we will leave the map and enter the terrain.
Having explored how these families connect conceptually, we will now study their mathematical foundations, estimation methods, and implementations ‚Äî the craft of making classification models work in practice.
 
----

# V. Estimation Methods and Model-Specific Analysis

## Purpose

Classification models are more than algorithms; they are expressions of how we understand uncertainty and structure decision boundaries in data.
This section forms the core analytical backbone of the repository.
Its goal is to explain, model by model, the logic of estimation‚Äîhow each technique learns, adapts, and makes predictions based on evidence.

In the previous section, we explored the taxonomy of classification models ‚Äî a conceptual map of the families and their philosophical foundations.
Now, we move from that map to the machinery: we will look inside each model, understanding how it transforms inputs into decisions.

Every model we study here represents a distinct way of thinking about the world: linearity and probability, margins and geometry, similarity and distance, hierarchy and ensemble collaboration, and, ultimately, deep representation.

This part remains purely theoretical and conceptual.
We will not write code or derive formulas step-by-step.
Instead, the goal is to achieve clarity and intuition ‚Äî to make the mathematics and training principles explain themselves.

All practical implementations, visualizations, and experiments will be developed later in the Practical Annexes (Sections VI‚ÄìVII), where we will connect theory to execution.

## Guiding Framework

To ensure consistency across all techniques, every model follows the same explanatory skeleton.
This uniform structure allows fair comparison between algorithms and helps build intuition layer by layer.

Each model will therefore be described through the following lenses:

**1. What is it?**
A short conceptual definition and its historical or disciplinary origin.

**2. Why use it?**
The typical scenarios, problems, or data structures where it excels.

**3. Intuition.**
The geometric, probabilistic, or algorithmic ‚Äúmental picture‚Äù of how it learns.

**4. Mathematical foundation.**
The key principle behind its estimation ‚Äî its objective function or decision rule ‚Äî explained in plain language and minimal notation.

**5. Training logic.**
A conceptual description of how parameters are adjusted to minimize loss or maximize separation.

**6. Assumptions and limitations.**
The data conditions that must hold for the model to perform well, and where it tends to fail.

**7. Key hyperparameters (conceptual view).**
Explanation of the main parameters that govern flexibility, bias‚Äìvariance balance, and generalization capacity.

**8. Evaluation focus.**
The most relevant metrics and diagnostic strategies to assess performance (linked to Section III on cross-cutting topics).

**9. When to use / When not to use.**
Guidance on appropriate contexts and common misuses.

**10. References.**
Three foundational academic sources and two reputable web resources for further study.

## Transition to the Model Families

Before diving into individual techniques, it is important to recognize that no model exists in isolation.
Each family of classifiers represents a historical and conceptual response to the limitations of the previous ones.
From linear equations to deep neural networks, the evolution of classification has been a continuous dialogue between simplicity and capacity, interpretability and flexibility, data scarcity and abundance.

We will now follow that natural order ‚Äî the same logic established in the previous section‚Äôs taxonomy ‚Äî beginning with the most fundamental of all: Linear and Probabilistic Models.

These methods form the bedrock of statistical learning.
They taught machines to make decisions not by memorizing data, but by estimating probabilities and drawing boundaries that separate uncertainty into meaning.

## Canon of Models Covered (by Family)

The diversity of classification models can seem overwhelming at first glance.
Each algorithm has its own philosophy, mathematical logic, and trade-off between interpretability and predictive power.
To make this exploration coherent, we will organize all techniques into families that share a common foundation ‚Äî linear reasoning, geometric margins, similarity, hierarchical splitting, collective learning, and deep representation.

This ‚Äúcanon of models‚Äù mirrors the conceptual order established in the Taxonomy (Section IV), so that we move naturally from the simplest to the most advanced ideas, from probability to representation, from equations we can visualize to networks we can only approximate.

Every model, regardless of its family, will be developed using the same analytical lens:

‚Ä¢	What is it?  ‚Üí conceptual definition and short historical note.
	
‚Ä¢	Why use it?  ‚Üí main use cases and decision context.
	
‚Ä¢	Intuition.  ‚Üí mental model of how the classifier separates or estimates.
	
‚Ä¢	Mathematical foundation.  ‚Üí principle of estimation or loss function explained in plain terms.
	
‚Ä¢	Training logic.  ‚Üí how the algorithm learns from data conceptually.
	
‚Ä¢	Assumptions & limitations.  ‚Üí when the model‚Äôs logic holds, and when it breaks.
	
‚Ä¢	Key hyperparameters.  ‚Üí parameters that shape its flexibility and generalization.
	
‚Ä¢	Evaluation focus.  ‚Üí metrics that best reflect success for this model.
	
‚Ä¢	When to use / When not to use.  ‚Üí practical guidance.
	
‚Ä¢	References.  ‚Üí canonical academic and web sources.
	

This uniform structure ensures that readers can compare models fairly, understand their conceptual genealogy, and recognize that there is no single ‚Äúbest‚Äù classifier ‚Äî only the right one for a given problem and context.

### A. Linear & Probabilistic Models

Linear and probabilistic models represent the foundation of supervised classification.
They are rooted in the idea that decision boundaries can be expressed as linear functions of the input variables, and that uncertainty can be modeled probabilistically.

These models emerged from the intersection of statistics and early pattern recognition in the mid-20th century.
Rather than memorizing examples, they learn relationships between features and outcomes by estimating parameters that maximize the likelihood of observing the data ‚Äî a statistical view of learning that precedes modern machine learning.

Their key strength lies in interpretability and mathematical elegance.
Each coefficient tells a story about how a variable contributes to the final decision, and probabilities offer a natural way to express confidence.
They perform remarkably well when the relationships in the data are approximately linear and the signal-to-noise ratio is moderate.

However, their simplicity is also their main limitation.
When decision boundaries are nonlinear or interactions between features are complex, linear and probabilistic models may struggle ‚Äî paving the way for more flexible approaches such as margin-based and tree-based learners.

In this family, we will explore five major techniques that together define the statistical heart of classification:

1.	Logistic Regression (binary, multinomial) ‚Äì the cornerstone of probabilistic classification, modeling the log-odds of class membership.

2.	Regularized Logistic Regression (L1, L2, Elastic Net) ‚Äì adding control over model complexity through penalty terms.

3.	Linear Discriminant Analysis (LDA) ‚Äì separating classes by maximizing between-class variance.

4.	Quadratic Discriminant Analysis (QDA) ‚Äì extending LDA with distinct covariance structures for each class.

5.	Naive Bayes (Gaussian, Multinomial, Bernoulli, Complement) ‚Äì a probabilistic learner based on independence assumptions and Bayes‚Äô theorem.

Together, these models form the analytical backbone of classical classification ‚Äî the bridge between pure statistics and the more flexible, data-driven methods that followed.

#### ** 1.Logistic Regression (binary, multinomial)**

**What is it?**

Logistic Regression is the most fundamental probabilistic model for classification.
Despite its name, it is not a regression method in the traditional sense but a predictive model for categorical outcomes.
It estimates the probability that an observation belongs to a particular class by modeling the relationship between the input features and the log-odds of the event.

The model originated in the early 20th century through the work of David Cox (1958), who extended earlier logit models from statistics and epidemiology to binary outcomes.
It later became a cornerstone of statistical learning theory, serving as the conceptual bridge between linear regression and modern classification algorithms.

‚∏ª

**Why use it?**

Logistic Regression is used when the goal is to classify observations into discrete categories (e.g., ‚Äúyes/no‚Äù, ‚Äúfraud/not fraud‚Äù, ‚Äúdisease/healthy‚Äù) while also quantifying the confidence of those classifications.
It is especially valuable in applications where interpretability and probability calibration matter as much as accuracy ‚Äî such as medicine, finance, and social sciences.

Typical use cases include:
	‚Ä¢	Predicting whether a patient has a disease given test results.
	‚Ä¢	Determining if a transaction is fraudulent.
	‚Ä¢	Estimating whether a customer will churn.
	‚Ä¢	Modeling the probability of an event (success/failure, default/no default).

Its transparency makes it ideal for regulated industries, where decision-making must be explainable and auditable.

‚∏ª

**Intuition**

At its heart, Logistic Regression asks a simple question:

‚ÄúGiven this input, how likely is it that the outcome is 1 instead of 0?‚Äù

Imagine a straight line (in one dimension) or a plane (in multiple dimensions) dividing two classes.
Instead of producing a hard decision, the model computes a smooth curve ‚Äî the logistic function ‚Äî that transforms any linear combination of inputs into a probability between 0 and 1.

In geometric terms, Logistic Regression learns a linear boundary in the feature space ‚Äî the point where the model is equally uncertain about both classes (probability = 0.5).
In probabilistic terms, it models the log-odds of belonging to one class as a linear function of the inputs:

$$
\log \left( \frac{P(y = 1 \mid x)}{1 - P(y = 1 \mid x)} \right) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p
$$

The logistic (sigmoid) transformation then converts these log-odds back into probabilities:

$$
P(y = 1 \mid x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + \dots + \beta_p x_p)}}
$$

This combination of linearity and nonlinearity ‚Äî a linear function wrapped by a nonlinear sigmoid ‚Äî is what makes Logistic Regression both simple and powerful.

üí° Note:
A closely related model, the Probit Regression, replaces the logistic (sigmoid) function with the cumulative normal distribution.
Both aim to map linear predictors into probabilities, differing mainly in their link functions.
The logit is preferred for interpretability and computational simplicity, but the probit offers similar results and can be used as an alternative in classification contexts.

‚∏ª

**Mathematical foundation**

The core principle is Maximum Likelihood Estimation (MLE).
Given a dataset with binary outcomes y_i \in \{0,1\} and predictors x_i,
the model estimates coefficients \beta that maximize the likelihood of observing the data:

$$
L(\beta) = \prod_{i=1}^{n} P(y_i \mid x_i) = \prod_{i=1}^{n} [p_i]^{y_i} [1 - p_i]^{1 - y_i}
$$

where

$$
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta^T x_i)}}
$$

The optimization is typically performed by minimizing the log-loss (negative log-likelihood):

$$
\text{Loss}(\beta) = - \sum_{i=1}^{n} \Big[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \Big]
$$

This formulation naturally penalizes confident but wrong predictions more heavily than uncertain ones,
encouraging models that are both accurate and probabilistically well-calibrated.

For multinomial Logistic Regression, the model generalizes to multiple classes using the softmax function,
assigning probabilities to each possible outcome while ensuring they sum to 1:

$$
P(y = k \mid x) = \frac{e^{\beta_k^T x}}{\sum_{j=1}^{K} e^{\beta_j^T x}}
$$

‚∏ª

**Training logic**

Training involves iteratively adjusting coefficients to minimize the log-loss.
Because the loss function is convex, gradient-based methods such as Newton‚ÄìRaphson, Iteratively Reweighted Least Squares (IRLS), or Stochastic Gradient Descent (SGD) are guaranteed to converge to a global minimum.

Conceptually, each iteration performs three steps:
	1.	Compute predicted probabilities from the current coefficients.
	2.	Measure how far predictions are from true outcomes (residuals).
	3.	Adjust coefficients proportionally to the direction of greatest improvement (the gradient).

This process continues until the changes in coefficients are small enough to indicate convergence.
The model‚Äôs simplicity makes training stable, efficient, and reproducible, even for moderately large datasets.

‚∏ª

**Assumptions and limitations**

Logistic Regression is powerful but relies on several assumptions:
	‚Ä¢	Linearity in the log-odds: the relationship between predictors and the logit of the outcome must be linear.
	‚Ä¢	Independent observations: errors across samples should be uncorrelated.
	‚Ä¢	No extreme multicollinearity: predictors should not be highly correlated, as this destabilizes coefficients.
	‚Ä¢	Sufficient sample size: large enough to estimate reliable probabilities.

Limitations include:
	‚Ä¢	Poor performance when decision boundaries are nonlinear.
	‚Ä¢	Sensitivity to outliers and missing values.
	‚Ä¢	Difficulty capturing complex feature interactions without manual feature engineering.

Nonetheless, these same constraints make it highly interpretable, a quality often lost in more complex algorithms.

‚∏ª

**Key hyperparameters (conceptual view)**

Although Logistic Regression is mathematically straightforward, several configuration choices influence its behavior:
	‚Ä¢	Regularization strength (C or Œª): controls overfitting by shrinking coefficients.
	‚Ä¢	Penalty type (L1, L2, Elastic Net): determines how regularization is applied (sparse vs smooth solutions).
	‚Ä¢	Solver: optimization algorithm (e.g., "liblinear", "lbfgs", "saga").
	‚Ä¢	Class weights: rebalance the influence of minority classes in imbalanced datasets.

These hyperparameters govern the bias‚Äìvariance trade-off, balancing simplicity and generalization.

‚∏ª

**Evaluation focus**

Because Logistic Regression produces probabilities, it should be evaluated not only for classification accuracy but also for probability calibration ‚Äî
how well predicted probabilities match actual observed frequencies.

Key metrics include:
	‚Ä¢	Log-loss: direct measure of probabilistic accuracy.
	‚Ä¢	ROC‚ÄìAUC: overall discrimination power between classes.
	‚Ä¢	PR‚ÄìAUC: preferred in imbalanced classification tasks.
	‚Ä¢	Brier score: calibration and reliability of predicted probabilities.
	‚Ä¢	Confusion matrix & F1-score: evaluation under chosen decision thresholds.

Interpreting coefficient signs and magnitudes adds a qualitative layer ‚Äî
connecting statistical learning with domain understanding.

‚∏ª

**When to use / When not to use**

Use Logistic Regression when:
	‚Ä¢	The relationship between predictors and the outcome is roughly linear.
	‚Ä¢	You need interpretable coefficients and well-calibrated probabilities.
	‚Ä¢	The dataset is of moderate size and not excessively high-dimensional.
	‚Ä¢	Transparency and explainability are priorities (e.g., healthcare, public policy, credit scoring).

Avoid Logistic Regression when:
	‚Ä¢	Data exhibit strong nonlinearities or complex feature interactions.
	‚Ä¢	Predictors are highly correlated or numerous relative to observations.
	‚Ä¢	Decision boundaries are highly irregular or discontinuous.
	‚Ä¢	You prioritize pure predictive accuracy over interpretability.

‚∏ª

**References**

Canonical papers
	1.	Cox, D. R. (1958). The Regression Analysis of Binary Sequences. Journal of the Royal Statistical Society, Series B.
	2.	McCullagh, P. & Nelder, J. (1989). Generalized Linear Models (2nd ed.). Chapman & Hall.
	3.	Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). Applied Logistic Regression (3rd ed.). Wiley.

Web resources

1.	StatQuest ‚Äì Logistic Regression Clearly Explained
	https://statquest.org/video/logistic-regression-clearly-explained/
3.	Scikit-learn User Guide: Logistic RegressionÔøº
	https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression

‚∏ª

----

Logistic Regression established the foundation for probabilistic classification ‚Äî
a world where decisions are guided by likelihood and evidence rather than hard rules.
Yet, its linear boundary can only go so far.
As data became more complex, new methods emerged to control overfitting and handle correlated or numerous predictors.

The next step in our journey explores how Regularized Logistic Regression extends this model ‚Äî
adding flexibility without losing interpretability, through the elegant mathematics of penalization.

----

#### 2. Regularized Logistic Regression (L1, L2, Elastic Net)

What is it?

Regularized Logistic Regression is an enhanced version of traditional Logistic Regression that adds a penalty term to the loss function to control model complexity.
While standard logistic regression seeks coefficients that perfectly fit the data, regularization constrains them to remain small or sparse ‚Äî improving generalization, stability, and interpretability.

This idea emerged from the evolution of penalized likelihood methods in the late 20th century, especially from the work on ridge regression (Hoerl & Kennard, 1970) and Lasso (Tibshirani, 1996).
By integrating these penalties into logistic regression, statisticians and data scientists obtained a model that balances fit and simplicity, preventing overfitting in high-dimensional or correlated datasets.

‚∏ª

Why use it?

Regularized Logistic Regression is preferred when:
	‚Ä¢	You have many predictors or potential multicollinearity.
	‚Ä¢	The model overfits the training data.
	‚Ä¢	You want automatic feature selection (especially with L1).
	‚Ä¢	You need better stability and generalization without losing interpretability.

Common applications:
	‚Ä¢	Credit scoring with dozens of financial indicators.
	‚Ä¢	Text or NLP classification (with many sparse features).
	‚Ä¢	Biomedical studies where predictors are correlated (e.g., genetic markers).
	‚Ä¢	Marketing models where variable selection is needed.

‚∏ª

Intuition

Regularization is like a gentle constraint placed on the model ‚Äî
it says: ‚Äúfit the data well, but don‚Äôt overreact.‚Äù

In standard logistic regression, coefficients can grow large to accommodate small patterns or noise.
Regularization keeps them small or pushes some to zero (in the case of L1), which simplifies the model and improves its ability to generalize.

Imagine tuning a musical instrument:
	‚Ä¢	Without regularization, each string (feature) vibrates freely, sometimes creating noise.
	‚Ä¢	With regularization, you tighten them just enough to maintain harmony ‚Äî a cleaner, more stable sound.

In geometric terms, regularization reshapes the optimization landscape:
	‚Ä¢	L2 (Ridge) uses circular (Euclidean) constraints, shrinking all coefficients smoothly.
	‚Ä¢	L1 (Lasso) uses diamond-shaped constraints, which naturally ‚Äúcut‚Äù some coefficients to zero.
	‚Ä¢	Elastic Net blends both worlds ‚Äî it shrinks most coefficients (L2) but can also eliminate the weakest (L1).

‚∏ª

Mathematical foundation

Regularized Logistic Regression minimizes the penalized log-loss function, balancing model fit and coefficient shrinkage:

$$
\text{Loss}{\text{reg}}(\beta)
= - \sum{i=1}^{n} \Big[ y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \Big]
	‚Ä¢	\lambda , P(\beta)
$$

where

$$
p_i = \frac{1}{1 + e^{-(\beta_0 + \beta^T x_i)}}
$$

and P(\beta) is the penalty term that depends on the chosen regularization type:

‚∏ª

L1 (Lasso)
$$
P(\beta) = \sum_{j=1}^{p} |\beta_j|
$$

Encourages sparsity by forcing irrelevant coefficients to zero ‚Äî effectively performing automatic variable selection.

‚∏ª

L2 (Ridge)
$$
P(\beta) = \sum_{j=1}^{p} \beta_j^2
$$

Shrinks all coefficients smoothly toward zero, improving stability and reducing the impact of multicollinearity.

‚∏ª

Elastic Net
$$
P(\beta) = \alpha \sum_{j=1}^{p} |\beta_j|
	‚Ä¢	(1 - \alpha) \sum_{j=1}^{p} \beta_j^2
$$

Combines both penalties, with
\alpha \in [0,1] controlling the balance between sparsity (L1) and smoothness (L2).

When \alpha = 1, the model behaves like pure Lasso;
when \alpha = 0, it behaves like pure Ridge.

‚∏ª

The regularization strength \lambda determines how strongly the penalty term constrains the coefficients:
	‚Ä¢	Large \lambda ‚Üí stronger penalty ‚Üí simpler model (higher bias, lower variance).
	‚Ä¢	Small \lambda ‚Üí weaker penalty ‚Üí behaves like standard logistic regression.

Thus, \lambda acts as a bias‚Äìvariance control knob, letting the analyst trade precision for stability depending on the problem‚Äôs complexity and sample size.
Training logic

The training process is similar to ordinary logistic regression but includes the regularization term in the optimization objective.
Because the penalty can make the function non-differentiable (especially with L1), solvers use coordinate descent, SGD, or proximal gradient methods to find the optimal coefficients.

The iterative logic can be summarized as:

1.	Compute predicted probabilities using the current coefficients.

2.	Calculate the gradient of the loss plus the penalty.

3.	Update coefficients in the opposite direction of the gradient, adjusted by the learning rate.

4.	For L1 penalties, coefficients that shrink below a threshold become exactly zero.

This training approach ensures stability and convergence, even for large or sparse datasets.

‚∏ª

Assumptions and limitations

The assumptions remain mostly the same as for standard logistic regression:

‚Ä¢	Linearity in the log-odds.

‚Ä¢	Independence of observations.

‚Ä¢	No severe outliers or missingness.

However, regularization relaxes the requirement of uncorrelated predictors, as L2 helps stabilize correlated variables and L1 can remove redundant ones.

Limitations:
	‚Ä¢	Choice of Œª and Œ± is critical ‚Äî too high can underfit, too low can overfit.
	‚Ä¢	Coefficients lose their direct interpretability when heavily regularized.
	‚Ä¢	L1 may behave unstably when predictors are highly correlated (Elastic Net often helps).

‚∏ª

Key hyperparameters (conceptual view)

‚Ä¢	Œª (Regularization strength): controls the trade-off between fit and simplicity.
Larger Œª increases the penalty, leading to smaller coefficients.

‚Ä¢	Penalty type:

‚Ä¢	"l1" for Lasso (sparse model).

‚Ä¢	"l2" for Ridge (smooth shrinkage).

‚Ä¢	"elasticnet" for a combination.

‚Ä¢	Œ± (Elastic Net mixing parameter): balances L1 and L2 penalties (Œ±=1 ‚Üí Lasso, Œ±=0 ‚Üí Ridge).

‚Ä¢	Solver: must support the chosen penalty (e.g., "liblinear" for L1, "saga" for Elastic Net).

‚Ä¢	Class weights: optionally adjust for imbalanced data.

These parameters define how much regularization is applied and which type of structure is favored in the solution.

‚∏ª

Evaluation focus

The evaluation metrics are the same as for ordinary logistic regression (log-loss, ROC‚ÄìAUC, PR‚ÄìAUC, Brier score),
but special attention should be given to bias‚Äìvariance trade-offs and feature selection stability.

When tuning Œª:
	‚Ä¢	Track both training and validation log-loss curves to avoid under/overfitting.
	‚Ä¢	Use cross-validation (e.g., k-fold CV) to find the optimal Œª.
	‚Ä¢	Examine which variables remain active (non-zero) ‚Äî this provides interpretability insights.

üí° Tip:
In practical applications, Elastic Net often performs best when the number of features is large and correlated ‚Äî
offering the robustness of Ridge and the feature selection power of Lasso.

‚∏ª

When to use / When not to use

Use Regularized Logistic Regression when:

‚Ä¢	You have many predictors (high dimensionality).

‚Ä¢	Variables are correlated or redundant.

‚Ä¢	You need better generalization than standard logistic regression.

‚Ä¢	You want to perform embedded feature selection.

Avoid it when:

‚Ä¢	Interpretability of individual coefficients is critical (since penalties distort raw magnitudes).

‚Ä¢	The dataset is small and simple (standard logistic regression suffices).

‚Ä¢	The relationship between features and outcomes is strongly nonlinear (consider tree-based or kernel methods).

‚∏ª

References

Canonical papers

1.	Hoerl, A. E., & Kennard, R. W. (1970). Ridge Regression: Biased Estimation for Nonorthogonal Problems. Technometrics, 12(1), 55‚Äì67.
   
2.	Tibshirani, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1), 267‚Äì288.
   
3.	Zou, H., & Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, 67(2), 301‚Äì320.

Web resources

1.	StatQuest ‚Äì Ridge, Lasso, and Elastic Net RegressionÔøº

https://www.youtube.com/watch?v=Q81RR3yKn30

3.	Scikit-learn User Guide: Regularization in Logistic RegressionÔøº

https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression


----

Regularized Logistic Regression introduced discipline into linear models ‚Äî teaching them to resist noise, ignore irrelevant predictors, and focus on signal. Yet, it remains confined to the assumption that decision boundaries are linear in the feature space.

To go beyond that ‚Äî to model more complex, nonlinear separations ‚Äî we must leave the realm of pure probability and enter that of geometry and distance.

The next model family, Linear Discriminant Analysis (LDA),
embodies this shift: it keeps a probabilistic heart but builds its boundary using the geometry of variance and covariance: a bridge between statistics and pattern recognition.

----

#### 3. Linear Discriminant Analysis (LDA)

What is it?

Linear Discriminant Analysis (LDA) is one of the earliest and most elegant statistical techniques for classification.
It seeks to find a linear combination of features that best separates two or more classes.
Rather than predicting probabilities directly, LDA projects the data into a new space where the distance between class means is maximized while the variance within each class is minimized.

Originally introduced by R. A. Fisher (1936) in his work on iris flower classification, LDA has since become a fundamental method in pattern recognition, serving both as a classifier and a dimensionality reduction technique.

‚∏ª

Why use it?

LDA is ideal when:
	‚Ä¢	Classes are linearly separable or approximately so.
	‚Ä¢	You want a transparent model that provides insight into class structure.
	‚Ä¢	You have small to medium-sized datasets with well-behaved features (no extreme outliers or heavy nonlinearity).
	‚Ä¢	You need robust probabilistic classification under Gaussian assumptions.

It performs particularly well in:
	‚Ä¢	Medical diagnosis (e.g., distinguishing healthy vs. diseased patients).
	‚Ä¢	Marketing (predicting customer segment membership).
	‚Ä¢	Text classification (as a linear projection step).
	‚Ä¢	As a preprocessing stage before logistic regression, SVM, or neural networks.

‚∏ª

Intuition

Imagine plotting data from two classes (say, blue and red points) on a two-dimensional plane.
LDA tries to find the best line that separates those two clouds of points.
If projected onto that line, the distance between the class means is maximized, while the spread within each class is minimized.

Mathematically, it‚Äôs an optimization of signal-to-noise ratio:
the ‚Äúsignal‚Äù is the distance between class means, while the ‚Äúnoise‚Äù is the variance within each class.

LDA rotates and scales the space such that, in this new axis, classes are as distinct as possible ‚Äî
like aligning a camera to capture maximum separation between the groups.

‚∏ª

Mathematical foundation

LDA assumes that the data from each class k follows a multivariate normal distribution with class-specific means \mu_k but a common covariance matrix \Sigma.

The probability density function for class k is:

$$
P(x | y = k) = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}}
\exp\left(-\frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k)\right)
$$

Using Bayes‚Äô theorem, the posterior probability of class k is:

$$
P(y = k | x) = \frac{P(x | y = k) P(y = k)}{\sum_{l=1}^{K} P(x | y = l) P(y = l)}
$$

Taking the logarithm of the numerator gives the discriminant function:

$$
\delta_k(x) = x^T \Sigma^{-1} \mu_k - \frac{1}{2} \mu_k^T \Sigma^{-1} \mu_k + \log P(y = k)
$$

The model assigns an observation x to the class with the largest \delta_k(x).
Thus, the decision boundaries are linear, as they depend on linear combinations of x.

‚∏ª

Training logic
	1.	Compute the mean vector \mu_k for each class.
	2.	Compute the pooled covariance matrix \Sigma, assuming equal covariance across classes.
	3.	Estimate the prior probabilities P(y = k) from class frequencies.
	4.	Plug these estimates into the discriminant function \delta_k(x).
	5.	Classify each observation by the class with the highest discriminant score.

This process doesn‚Äôt require iterative optimization ‚Äî it is entirely analytical, making LDA fast, deterministic, and computationally efficient.

‚∏ª

Assumptions and limitations

Assumptions:
	‚Ä¢	Classes follow Gaussian (normal) distributions.
	‚Ä¢	Each class shares the same covariance matrix.
	‚Ä¢	Predictors are linearly related to the discriminant function.
	‚Ä¢	Observations are independent.

Limitations:
	‚Ä¢	Performance degrades when covariance structures differ substantially (use QDA instead).
	‚Ä¢	Sensitive to outliers and non-normal data.
	‚Ä¢	Cannot capture nonlinear boundaries.
	‚Ä¢	Requires more samples than features to estimate covariance reliably.

‚∏ª

Key hyperparameters (conceptual view)

Although LDA has few tunable parameters, each one subtly influences how the model behaves:
	‚Ä¢	priors ‚Üí define the prior probability of each class.
They adjust how much the classifier favors frequent or rare categories.
When priors are left unspecified, LDA automatically estimates them from the data.
	‚Ä¢	solver ‚Üí determines the computational approach used to estimate the discriminant directions.
The ‚Äúsvd‚Äù solver is the most common and numerically stable;
‚Äúlsqr‚Äù and ‚Äúeigen‚Äù are more suitable for large datasets or when shrinkage is applied.
	‚Ä¢	shrinkage ‚Üí introduces a small regularization term to the covariance matrix estimation.
This improves stability when the number of features approaches or exceeds the number of samples,
reducing overfitting in high-dimensional spaces.
	‚Ä¢	n_components ‚Üí specifies how many discriminant axes are retained.
While only up to K ‚Äì 1 axes are meaningful (where K is the number of classes),
limiting this parameter can be useful for visualization or as a preprocessing step for other models.

Each of these parameters balances stability, interpretability, and computational efficiency,
allowing LDA to adapt from small academic datasets to large applied problems.

‚∏ª

Evaluation focus

LDA‚Äôs performance is best assessed via:
	‚Ä¢	Accuracy, ROC‚ÄìAUC, and PR‚ÄìAUC for discriminative power.
	‚Ä¢	Confusion matrix to verify symmetry in misclassifications.
	‚Ä¢	Cross-validation stability, since covariance estimation may vary.
	‚Ä¢	Visualization of discriminant axes to assess separation quality.

Because it produces class probabilities, calibration and interpretability remain central evaluation points.

‚∏ª

When to use / When not to use

Use it when:
	‚Ä¢	The dataset is small to medium-sized and approximately Gaussian.
	‚Ä¢	You need a simple, fast, and interpretable linear classifier.
	‚Ä¢	Covariances between features are similar across classes.
	‚Ä¢	Dimensionality reduction is desired before another classifier.

Avoid it when:
	‚Ä¢	Covariance matrices differ significantly between classes (prefer QDA).
	‚Ä¢	The data distribution is highly skewed or nonlinear.
	‚Ä¢	There are too many features relative to samples (risk of singular covariance).

‚∏ª

References

Canonical papers
	1.	Fisher, R. A. (1936). The Use of Multiple Measurements in Taxonomic Problems. Annals of Eugenics.
	2.	Rao, C. R. (1948). The Utilization of Multiple Measurements in Problems of Biological Classification. Journal of the Royal Statistical Society.
	3.	McLachlan, G. J. (2004). Discriminant Analysis and Statistical Pattern Recognition. Wiley.

Web resources

1.	StatQuest: Linear Discriminant Analysis (LDA) Clearly Explained

 https://www.youtube.com/watch?v=azXCzI57Yfc
Ôøº
2.	Scikit-learn User Guide ‚Äî Linear Discriminant AnalysisÔøº

 https://scikit-learn.org/stable/modules/lda_qda.html


-----

Linear Discriminant Analysis provided a statistically elegant way to separate classes under Gaussian assumptions.
Yet, when those assumptions break ‚Äî when each class has its own covariance or when the relationship becomes curved rather than flat ‚Äî LDA begins to lose accuracy.

The natural evolution is Quadratic Discriminant Analysis (QDA),
which relaxes LDA‚Äôs most restrictive assumption by allowing each class to have its own covariance structure,
leading to nonlinear decision boundaries that can better capture complex class shapes.

-----


#### 4. Quadratic Discriminant Analysis (QDA)

Quadratic Discriminant Analysis (QDA) is the nonlinear extension of Linear Discriminant Analysis (LDA).
While LDA assumes that all classes share the same covariance matrix, QDA allows each class to have its own covariance structure.
This flexibility enables QDA to learn curved (quadratic) decision boundaries that adapt to more complex data distributions.

QDA remains a probabilistic generative model ‚Äî it models how each class generates data through a multivariate Gaussian distribution and then applies Bayes‚Äô rule to classify observations.
Historically, it evolved from Fisher‚Äôs discriminant work (1936) and later generalized by statisticians such as Rao and Friedman.

‚∏ª

Why use it?

QDA is useful when the covariance structure of each class differs ‚Äî that is, when the spread, orientation, or shape of the class clouds in feature space is not homogeneous.
It performs particularly well in:
	‚Ä¢	Biomedical and diagnostic problems where patient groups have different variability.
	‚Ä¢	Fault detection or signal analysis where data dispersion changes by category.
	‚Ä¢	Any domain where nonlinear class boundaries are needed but interpretability is still desired.

In short, QDA trades some simplicity for richer geometric representation.

‚∏ª

Intuition

Imagine each class as an ellipsoid cloud of points in feature space.
LDA fits one shared ellipse to all classes, separating them with straight lines (planes).
QDA instead fits a separate ellipse per class, allowing boundaries that bend to follow each class‚Äôs natural contour.

At prediction time, QDA evaluates how likely a new point is under each class‚Äôs Gaussian ‚Äúshape.‚Äù
The observation is assigned to the class where it falls within the highest probability region.

‚∏ª

Mathematical foundation

Each class k is assumed to follow a multivariate normal distribution with its own mean vector \mu_k and covariance matrix \Sigma_k:

$$
P(x \mid y = k) =
\frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}}
\exp!\left(
-\tfrac{1}{2}(x - \mu_k)^{T} \Sigma_k^{-1} (x - \mu_k)
\right)
$$

Using Bayes‚Äô theorem, the posterior probability of class k is proportional to:

$$
P(y = k \mid x) \propto
P(x \mid y = k) P(y = k)
$$

Taking logarithms (and omitting constants that are equal for all classes) yields the discriminant function:

$$
\delta_k(x)
= -\tfrac{1}{2} \log |\Sigma_k|
-\tfrac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k)
	‚Ä¢	\log P(y = k)
$$

The model assigns x to the class with the highest \delta_k(x).
Because \Sigma_k differs across classes, the resulting decision boundaries are quadratic surfaces ‚Äî hyper-ellipses rather than hyperplanes.

‚∏ª

Training logic

Training QDA involves closed-form estimation, not iterative optimization:
	1.	Compute the class means \mu_k.
	2.	Compute the class-specific covariance matrices \Sigma_k.
	3.	Estimate prior probabilities P(y = k) from class frequencies or predefined priors.
	4.	Plug these estimates into the discriminant function \delta_k(x).
	5.	Classify each observation by selecting the class with the maximum discriminant score.

This makes QDA analytically elegant but computationally heavier than LDA because each class requires a full covariance estimate.

‚∏ª

Assumptions and limitations

Assumptions:
	‚Ä¢	Each class follows a multivariate Gaussian distribution.
	‚Ä¢	Observations are independent and identically distributed.
	‚Ä¢	Sample size per class is large enough to estimate its covariance matrix reliably.

Limitations:
	‚Ä¢	When the number of features p is large relative to the number of samples n_k per class, covariance estimation can become unstable or singular.
	‚Ä¢	Sensitive to outliers and feature scaling.
	‚Ä¢	If class covariances are actually similar, LDA may generalize better because it pools information across classes.

Regularization or covariance shrinkage can partially mitigate these issues.

‚∏ª

Key hyperparameters (conceptual view)

QDA has few but crucial parameters that influence performance and stability:
	‚Ä¢	priors ‚Äî Define class prior probabilities P(y = k).
Adjusting priors changes how strongly the model favors frequent or rare classes.
	‚Ä¢	reg_param ‚Äî Adds a small multiple of the identity matrix to each covariance matrix,
controlling the trade-off between bias and variance. Larger values yield more regularized (spherical) shapes.
	‚Ä¢	store_covariance / tol ‚Äî Numerical options controlling precision, storage, and convergence tolerance.

In practice, the regularization parameter is the most important, as it stabilizes estimation when data are high-dimensional or imbalanced.

‚∏ª

Evaluation focus

Because QDA produces posterior probabilities, evaluation goes beyond accuracy.
Typical diagnostics include:
	‚Ä¢	Log-loss and Brier score for probabilistic calibration.
	‚Ä¢	ROC‚ÄìAUC and PR‚ÄìAUC for discrimination quality.
	‚Ä¢	Confusion matrix to inspect asymmetric misclassifications.
	‚Ä¢	Cross-validation stability across folds, especially in small datasets.

Visualizing decision boundaries in 2-D projections can also reveal whether the quadratic surfaces match intuition about class geometry.

‚∏ª

When to use / When not to use

Use QDA when:
	‚Ä¢	Each class has distinct covariance patterns.
	‚Ä¢	You have sufficient samples per class to estimate \Sigma_k.
	‚Ä¢	Curved decision boundaries are necessary.
	‚Ä¢	Probabilistic interpretability is still desired.

Avoid QDA when:
	‚Ä¢	The feature dimension is high relative to sample size (risk of overfitting).
	‚Ä¢	Covariances are approximately equal ‚Äî prefer LDA.
	‚Ä¢	Strong collinearity or outliers distort the covariance estimates.

Regularized or hybrid approaches (e.g., Friedman‚Äôs Regularized Discriminant Analysis) can act as intermediate solutions between LDA and QDA.

‚∏ª

References

Canonical papers
	1.	Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning, Chapter 4. Springer.
	2.	McLachlan, G. (2004). Discriminant Analysis and Statistical Pattern Recognition. Wiley.
	3.	Friedman, J. (1989). Regularized Discriminant Analysis. Journal of the American Statistical Association.

Web resources
	‚Ä¢	Scikit-learn User Guide ‚Äî Quadratic Discriminant Analysis
https://scikit-learn.org/stable/modules/lda_qda.html#quadratic-discriminant-analysisÔøº
	‚Ä¢	StatQuest ‚Äî LDA and QDA (video overview)
https://www.youtube.com/watch?v=EIJG0xHdl3kÔøº

------

Quadratic Discriminant Analysis extended linear boundaries into smooth curves,
offering a probabilistic yet flexible lens for complex class structures.
However, as the number of parameters grows with every covariance matrix,
QDA can quickly become unstable in high-dimensional spaces.

This limitation motivated a new family of algorithms ‚Äî
models that avoid estimating full distributions and instead focus on decision margins.
The next step in our journey introduces these margin-based learners, beginning with the Perceptron.

------

#### 5. Naive Bayes (Gaussian, Multinomial, Bernoulli, Complement)

What is it?

Naive Bayes is a family of simple yet remarkably effective probabilistic classifiers based on Bayes‚Äô theorem with the strong assumption that features are conditionally independent given the class label.
Despite this unrealistic ‚Äúnaive‚Äù assumption, it performs surprisingly well in many real-world problems, especially when features contribute additively to the decision.

Naive Bayes is part of the earliest generation of machine-learning algorithms, rooted in statistical inference and pattern recognition since the 1950s. It remains popular for text classification, spam detection, and document categorization because of its scalability and interpretability.

‚∏ª

Why use it?

Naive Bayes excels when:
	‚Ä¢	Data dimensionality is high (e.g., thousands of words or features).
	‚Ä¢	Feature dependencies are weak or approximately additive.
	‚Ä¢	The goal is to get fast, interpretable, and well-calibrated probabilities.

Typical applications:
	‚Ä¢	Email spam filtering (spam vs ham).
	‚Ä¢	Sentiment analysis in social media.
	‚Ä¢	Medical diagnosis from categorical symptoms.
	‚Ä¢	Document topic classification.

Its simplicity allows it to train in seconds even on very large datasets ‚Äî often outperforming more complex models in sparse domains.

‚∏ª

Intuition

At its heart, Naive Bayes computes:

‚ÄúGiven this input, which class makes the observed features most likely?‚Äù

Using Bayes‚Äô rule:

$$
P(y \mid x_1, x_2, \dots, x_p)
\propto
P(y),
P(x_1, x_2, \dots, x_p \mid y)
$$

The ‚Äúnaive‚Äù assumption decomposes the joint likelihood into independent terms:

$$
P(x_1, x_2, \dots, x_p \mid y)
= \prod_{j=1}^{p} P(x_j \mid y)
$$

This dramatically simplifies computation.
Each feature contributes individually to the overall likelihood, and the class with the highest posterior probability is predicted:

$$
\hat{y} = \arg\max_y ; P(y), \prod_{j=1}^{p} P(x_j \mid y)
$$

‚∏ª

Mathematical foundation

Different Naive Bayes variants differ only in how they model P(x_j \mid y):

1. Gaussian Naive Bayes
Used for continuous numeric features.
Each conditional feature distribution is Gaussian:

$$
P(x_j \mid y = k)

\frac{1}{\sqrt{2\pi\sigma_{jk}^2}}
\exp!\left(
-\frac{(x_j - \mu_{jk})^2}{2\sigma_{jk}^2}
\right)
$$

The class-conditional log-posterior (up to constants) is:

$$
\log P(y = k \mid x)
\propto
\log P(y = k)
-\frac{1}{2} \sum_{j} \frac{(x_j - \mu_{jk})^2}{\sigma_{jk}^2}
$$

2. Multinomial Naive Bayes
Used for count data (e.g., word frequencies in text).
The likelihood assumes a multinomial distribution:

$$
P(x \mid y = k)

\frac{(\sum_j x_j)!}{\prod_j x_j!}
\prod_{j=1}^{p} \theta_{jk}^{x_j}
$$

where \theta_{jk} = P(\text{feature } j \mid y = k).
Laplace (add-one) smoothing prevents zero probabilities.

3. Bernoulli Naive Bayes
Used for binary indicator features (presence/absence).

$$
P(x_j \mid y = k)

\theta_{jk}^{x_j},
(1 - \theta_{jk})^{(1 - x_j)}
$$

This variant captures whether a term appears rather than how many times.

4. Complement Naive Bayes
Designed for imbalanced text classification.
It estimates feature weights using statistics from complementary classes (all classes except the target) to reduce bias toward frequent categories.
This often improves performance when certain classes dominate the training data.

‚∏ª

Training logic

Training Naive Bayes is non-iterative and fully analytical:
	1.	Compute prior probabilities
P(y = k) = \frac{n_k}{n}.
	2.	Estimate conditional distributions
P(x_j \mid y = k) using counts (Multinomial/Bernoulli) or mean/variance (Gaussian).
	3.	Apply smoothing (Laplace or Lidstone) to avoid zero probabilities.
	4.	Store these estimates ‚Äî classification is just a lookup and product of probabilities.

Its training complexity is O(n \times p), making it among the fastest learning algorithms available.

‚∏ª

Assumptions and limitations

Assumptions
	‚Ä¢	Conditional independence of features given the class.
	‚Ä¢	Feature distributions follow the assumed model (Gaussian, Multinomial, etc.).
	‚Ä¢	Sufficient sample size per class to estimate reliable probabilities.

Limitations
	‚Ä¢	Independence rarely holds perfectly; correlations between features can degrade accuracy.
	‚Ä¢	Sensitive to how continuous data are modeled ‚Äî Gaussian assumption may be too restrictive.
	‚Ä¢	Probabilities can be poorly calibrated when independence is violated.

Still, the model is robust, and independence violations often do not destroy predictive power.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	alpha (Œ±) ‚Äî Smoothing parameter for avoiding zero probabilities.
	‚Ä¢	Œ± = 1 corresponds to Laplace smoothing.
	‚Ä¢	Œ± ‚Üí 0 removes smoothing (can cause instability).
	‚Ä¢	fit_prior ‚Äî Whether to learn class priors from data or assume uniform priors.
	‚Ä¢	var_smoothing (Gaussian NB) ‚Äî Small constant added to variance estimates to avoid division by zero or numerical instability.
	‚Ä¢	binarize (Bernoulli NB) ‚Äî Threshold value to transform numeric features into binary indicators.

These parameters control smoothness, stability, and robustness to rare features.

‚∏ª

Evaluation focus

Since Naive Bayes outputs probabilities, we assess both discrimination and calibration:
	‚Ä¢	Accuracy and F1-score for balanced datasets.
	‚Ä¢	Precision/Recall and PR-AUC for imbalanced text data.
	‚Ä¢	Log-loss and Brier score to measure probabilistic reliability.
	‚Ä¢	Confusion matrix to inspect systematic class bias.

Visualization of class posteriors or feature likelihoods often reveals where the independence assumption breaks down.

‚∏ª

When to use / When not to use

Use it when:
	‚Ä¢	You need a fast, interpretable baseline.
	‚Ä¢	Data are high-dimensional, sparse, or text-based.
	‚Ä¢	You prefer a model that works well with small training data.

Avoid it when:
	‚Ä¢	Features are strongly correlated (e.g., pixel intensities in images).
	‚Ä¢	You need complex, nonlinear decision boundaries.
	‚Ä¢	Feature distributions deviate substantially from the assumed form.

Despite these caveats, Naive Bayes often provides a competitive baseline that is hard to beat in efficiency and simplicity.

‚∏ª

References

Canonical papers
	1.	Domingos, P., & Pazzani, M. (1997). On the Optimality of the Simple Bayesian Classifier under Zero‚ÄìOne Loss. Machine Learning.
	2.	Mitchell, T. (1997). Machine Learning. McGraw-Hill.
	3.	Rennie, J. D. M., Shih, L., Teevan, J., & Karger, D. R. (2003). Tackling the Poor Assumptions of Naive Bayes Text Classifiers. ICML 2003.

Web resources
	‚Ä¢	Scikit-learn User Guide ‚Äî Naive Bayes
https://scikit-learn.org/stable/modules/naive_bayes.htmlÔøº
	‚Ä¢	StatQuest ‚Äî Naive Bayes Clearly Explained (video)
https://www.youtube.com/watch?v=O2L2Uv9pdDAÔøº


----

Naive Bayes completes our exploration of linear and probabilistic classifiers ‚Äî models that reason through likelihood and evidence.
However, these techniques rely heavily on assumptions about distributions and independence.

The next stage in our journey abandons those assumptions, replacing probability with geometry.
We move now to Margin-based Models, beginning with the Perceptron, the first algorithm to learn a separating hyperplane from data.

----

### B. Margin-based Models

The models explored so far ‚Äî Logistic Regression, LDA, QDA, and Naive Bayes ‚Äî interpret classification through the lens of probability.
They assume that data follow certain statistical patterns: distributions (often Gaussian), independence between features, and additive effects that together define the likelihood of each class.
These methods work beautifully when their assumptions are approximately true, offering interpretability, calibrated probabilities, and direct links to statistical theory.
Yet, as data became richer and more irregular, those assumptions began to constrain rather than empower.

Margin-based models emerged as a paradigm shift.
Instead of asking ‚ÄúWhich class is most probable given this point?‚Äù, they ask

‚ÄúWhere should the boundary lie so that classes are best separated?‚Äù

This shift replaces the probabilistic framework with a geometric one.
A classifier is now seen as a surface in feature space ‚Äî a hyperplane that divides points of different labels with the widest possible margin between them.
Each training sample exerts a geometric ‚Äúpull‚Äù on that boundary; the model learns by balancing these opposing forces until the separation is maximized.

Why this matters
	1.	Freedom from distributional assumptions
Margin-based algorithms do not require features to be Gaussian, independent, or even linearly correlated.
They rely solely on the geometry of the data ‚Äî distances and orientations ‚Äî making them robust in heterogeneous, high-dimensional environments.
	2.	Focus on boundaries, not densities
Probabilistic models approximate how data are distributed within each class.
Margin-based models focus on where classes meet, learning decision surfaces that adapt even when densities overlap or are non-parametric.
	3.	Better generalization via margins
Maximizing the separation between classes naturally reduces overfitting.
A larger margin implies that the model commits only when evidence is strong, yielding smoother and more stable decision regions.
	4.	Gateway to modern geometric learning
This concept of margins and separating hyperplanes became the foundation for Support Vector Machines and, indirectly, for many modern neural methods that also learn hierarchical geometric boundaries.

In summary

If probabilistic models represent reasoning under uncertainty,
margin-based models embody decision through geometry.
They define classification not as an act of inference but as the art of drawing the clearest possible line between competing explanations.

With that new lens, we now step into the first and most historic of these geometric learners ‚Äî the Perceptron, the algorithm that first taught machines how to learn a boundary from experience.

#### 6. Perceptron

What is it?

The Perceptron is the earliest and simplest algorithm for supervised classification based purely on geometry.
Introduced by Frank Rosenblatt (1958), it represents one of the first attempts to make a machine learn from experience ‚Äî by adjusting a decision boundary through exposure to data.

It learns a linear separator between two classes by iteratively updating weights whenever a sample is misclassified.
Though simple, the Perceptron introduced the core idea that still underlies modern neural networks: a neuron that combines inputs, applies a transformation, and outputs a decision.

‚∏ª

Why use it?

The Perceptron is used mainly for linearly separable classification problems and as a conceptual foundation for more advanced models such as Support Vector Machines (SVMs) and Artificial Neural Networks.

Its main advantages are:
	‚Ä¢	Simplicity: the training algorithm is intuitive and fast.
	‚Ä¢	Interpretability: the learned weights define the orientation of the separating hyperplane.
	‚Ä¢	Historical and pedagogical value: it teaches the principles of iterative learning and convergence.

Typical use cases are educational examples, prototype experiments, or low-dimensional datasets where a linear boundary suffices.

‚∏ª

Intuition

Geometrically, the Perceptron seeks a hyperplane that divides the input space into two halves ‚Äî one for each class.
Each sample exerts an influence: if a point is misclassified, the hyperplane is nudged in the direction that would classify it correctly next time.

The decision rule is based on the sign of a weighted linear combination of the features:

$$
\hat{y} = \text{sign}(w^T x + b)
$$

If the result is positive, the prediction is class +1; otherwise, it is ‚àí1.
Over many iterations, these corrections gradually align the hyperplane with the true boundary.

‚∏ª

Mathematical foundation

The Perceptron minimizes a simple misclassification loss by updating the weights whenever an error occurs.

For each sample (x_i, y_i), where y_i \in \{-1, +1\}:
	1.	Compute the prediction:
\hat{y}_i = \text{sign}(w^T x_i + b)
	2.	If the prediction is wrong (y_i \neq \hat{y}_i), update:

$$
w \leftarrow w + \eta, y_i, x_i
$$

$$
b \leftarrow b + \eta, y_i
$$

Here \eta (eta) is the learning rate ‚Äî a small constant controlling the step size.
The algorithm repeats until all points are correctly classified or a maximum number of iterations is reached.

Because this rule adjusts only on errors, the model naturally converges for linearly separable data.

‚∏ª

Training logic

Training is incremental and deterministic:
Each misclassified point ‚Äúteaches‚Äù the model by shifting the decision boundary toward the correct side.
The updates continue until either all points are correctly classified or the model cycles between a few errors (if the data are not linearly separable).

The Perceptron does not optimize a differentiable loss like modern gradient descent ‚Äî it performs direct corrections based on mistakes.
This simplicity makes it computationally efficient but also limits its applicability to problems where a perfect linear separator exists.

‚∏ª

Assumptions and limitations

Assumptions
	‚Ä¢	Data must be linearly separable (a single hyperplane can divide the classes).
	‚Ä¢	Features should be scaled or normalized for stable learning.

Limitations
	‚Ä¢	Fails to converge when data are not linearly separable.
	‚Ä¢	Sensitive to feature scaling and initialization.
	‚Ä¢	Produces hard decisions (no probabilities or confidence).
	‚Ä¢	Cannot handle multiclass problems natively (requires one-vs-rest extensions).

Despite these limits, its geometric simplicity made it the stepping stone for nearly all future linear classifiers.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	Learning rate (Œ∑): controls the step size of weight updates.
	‚Ä¢	Max iterations: prevents infinite loops when data are not separable.
	‚Ä¢	Shuffle or random seed: affects convergence order and stability.

These parameters influence how quickly and smoothly the model converges.

‚∏ª

Evaluation focus

Because it produces binary hard outputs, evaluation typically relies on:
	‚Ä¢	Accuracy and confusion matrix for balanced datasets.
	‚Ä¢	Precision, recall, and F1-score when classes are imbalanced.
	‚Ä¢	ROC‚ÄìAUC can be used when applying score-based variants (averaged raw activations).

The Perceptron‚Äôs performance is best interpreted geometrically ‚Äî by visualizing the boundary and the margin between classes.

‚∏ª

When to use / When not to use

Use it when:
	‚Ä¢	The classes are roughly linearly separable.
	‚Ä¢	You want a fast, interpretable, and educational model.
	‚Ä¢	The task involves small or low-dimensional data.

Avoid it when:
	‚Ä¢	The data are nonlinear or noisy.
	‚Ä¢	You need probabilistic outputs.
	‚Ä¢	Convergence or margin optimization matters (SVMs are superior in such cases).

‚∏ª

References

Canonical papers
	1.	Rosenblatt, F. (1958). The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review.
	2.	Minsky, M., & Papert, S. (1969). Perceptrons. MIT Press.
	3.	Novikoff, A. B. J. (1962). On Convergence Proofs on Perceptrons. Proceedings of the Symposium on the Mathematical Theory of Automata.

Web resources
	‚Ä¢	Scikit-learn User Guide ‚Äî Perceptron
https://scikit-learn.org/stable/modules/linear_model.html#perceptronÔøº
	‚Ä¢	StatQuest ‚Äî The Perceptron Clearly Explained (video)
https://www.youtube.com/watch?v=3Xc3CA655Y4Ôøº

-----

The Perceptron introduced the revolutionary idea of learning from errors ‚Äî updating a model dynamically as data arrive.
However, it lacked a notion of confidence and failed when classes were not perfectly separable.

This motivated the next step: algorithms that keep the geometric spirit of the Perceptron but seek the widest possible margin between classes.
The next model, the Support Vector Machine (SVM), formalizes that intuition into an elegant and mathematically powerful framework.

-----

#### 7. Linear SVM (soft margin, hinge loss)

**What is it?**

A Linear Support Vector Machine learns a separating hyperplane that maximizes the margin between classes. Instead of modeling class densities, it focuses on the decision boundary itself. The soft-margin variant adds slack to tolerate overlap and noise, which makes it practical for real data.

‚∏ª

**Why use it?**

It performs strongly on high-dimensional, sparse, or noisy problems (text, bag-of-words, wide tabular data). It often generalizes better than plain Perceptron or unregularized linear models because maximizing margin naturally controls overfitting. It is also robust when n << p.

‚∏ª

**Intuition**

Think of a line (or hyperplane) that separates classes while staying as far as possible from the closest points of both classes. Points that touch or violate the margin are the ‚Äúsupport vectors.‚Äù Only they determine the boundary; everything else is irrelevant for the final solution. Decisions come from the sign of a linear score:

$$
\hat{y} = \mathrm{sign}(w^\top x + b)
$$

‚∏ª

**Mathematical foundation**

Learning consists of balancing a large margin with few violations via the hinge loss.

Hinge loss (per sample) aggregates into the empirical risk:
$$
\mathcal{L}\text{hinge} = \sum{i=1}^{n} \max\big(0,; 1 - y_i (w^\top x_i + b)\big)
$$

Soft-margin primal objective:
$$
\min_{w,b,\xi}; \tfrac{1}{2},|w|^2 + C \sum_{i=1}^{n} \xi_i
$$

Constraints:
$$
\text{subject to } ; y_i (w^\top x_i + b) \ge 1 - \xi_i, \quad \xi_i \ge 0
$$

C controls the trade-off: large C penalizes violations more (lower bias, higher variance); small C allows a wider margin with more tolerance to errors (higher bias, lower variance).

‚∏ª

**Training logic**

Optimize the convex objective above (or an equivalent unconstrained hinge-loss form) with solvers such as coordinate descent, LIBLINEAR, or (stochastic) gradient methods on the primal. Only the support vectors (margin violators or exactly on the margin) matter for the final boundary.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	Classes are approximately linearly separable in the chosen feature space.
	‚Ä¢	Features are scaled (standardization strongly recommended).

**Limitations**
	‚Ä¢	Linear boundary only; complex curvature requires kernels (next section).
	‚Ä¢	Produces scores, not calibrated probabilities (apply Platt scaling or isotonic calibration if probabilities are needed; see Cross-cutting Topics).
	‚Ä¢	Sensitive to unscaled features and outliers with very large norm.

‚∏ª

**Key hyperparameters (conceptual view)**
	‚Ä¢	C: strength of penalty on margin violations; governs the bias‚Äìvariance trade-off.
	‚Ä¢	class_weight: compensates class imbalance by reweighting errors.
	‚Ä¢	max_iter / tol: convergence controls for the optimizer.
	‚Ä¢	fit_intercept: whether to estimate b; interacts with feature centering.

‚∏ª

**Evaluation focus**

Use ROC-AUC and PR-AUC on the raw decision scores for ranking quality. For hard decisions, inspect precision/recall/F1 at a chosen threshold. If you need probabilities (cost-sensitive decisions, risk ranking), calibrate the scores and assess log-loss or Brier score.

‚∏ª

**When to use / When not to use**

Use when features are many, linear signals are plausible, and you want a strong linear baseline with good generalization and robustness.
Avoid when boundaries are clearly nonlinear or interactions dominate; prefer Kernel SVM or tree/ensemble methods in those cases, or calibrate if probabilistic outputs are required.

‚∏ª

**References**

Canonical papers
	1.	Cortes, C., & Vapnik, V. (1995). Support-Vector Networks. Machine Learning.
	2.	Boser, B., Guyon, I., & Vapnik, V. (1992). A Training Algorithm for Optimal Margin Classifiers. COLT.
	3.	Hastie, Tibshirani, Friedman (2009). The Elements of Statistical Learning (Ch. 12). Springer.

Web resources**
	‚Ä¢	Scikit-learn User Guide ‚Äî Linear SVM (LinearSVC)
https://scikit-learn.org/stable/modules/svm.html#svm-classificationÔøº
	‚Ä¢	StatQuest ‚Äî SVMs Clearly Explained
https://www.youtube.com/watch?v=efR1C6CvhmEÔøº

------

Linear SVM gives us a strong boundary when linear signals dominate. When the data demand curved boundaries, we keep the SVM philosophy but map inputs into richer spaces via kernels. Next: Kernel SVM (RBF, polynomial).

------

#### 8. Kernel SVM (RBF, polynomial kernels)

**What is it?**

A Kernel Support Vector Machine (SVM) extends the linear SVM to handle non-linear decision boundaries.
It does so by implicitly mapping the input data into a higher-dimensional feature space where a linear separation becomes possible.
This transformation is achieved through a mathematical function called a kernel, which computes the similarity between points without explicitly performing the mapping.

This concept, known as the kernel trick, allows the SVM to learn complex, curved boundaries while maintaining the elegant geometric formulation of the linear case.

‚∏ª

**Why use it?**

Kernel SVMs are powerful for problems where the relationship between features and labels is non-linear but still structured ‚Äî meaning the classes can be separated by a smooth surface.
They are widely used in:
	‚Ä¢	Image and handwriting recognition (e.g., digits in MNIST).
	‚Ä¢	Bioinformatics (e.g., protein classification).
	‚Ä¢	Text categorization and sentiment analysis.

They often perform strongly even on small or medium-sized datasets, where deep neural networks would be unnecessary or prone to overfitting.

‚∏ª

**Intuition**

Instead of drawing a straight line, the model draws a smooth curved boundary by comparing every point with key examples (the support vectors).
It decides based on how similar a new observation is to these critical samples, as measured by the kernel function K(x_i, x_j).

The prediction is based on the sign of the weighted similarity sum:

$$
\hat{y} = \mathrm{sign}!\left(\sum_{i=1}^{n} \alpha_i y_i K(x_i, x) + b\right)
$$

Here the coefficients \alpha_i are found during training, and most of them are zero ‚Äî only the support vectors remain ‚Äúactive.‚Äù

‚∏ª

**Mathematical foundation**

The optimization problem is the dual form of the soft-margin SVM, where the kernel replaces the inner product:

$$
\max_{\alpha} \sum_{i=1}^{n} \alpha_i - \tfrac{1}{2} \sum_{i,j=1}^{n} \alpha_i \alpha_j y_i y_j K(x_i, x_j)
$$

Subject to:

$$
0 \le \alpha_i \le C, \quad \sum_{i=1}^{n} \alpha_i y_i = 0
$$

The kernel defines the geometry of the transformed space.
Common choices include:
	‚Ä¢	Linear kernel: K(x_i, x_j) = x_i^\top x_j
(equivalent to the linear SVM).
	‚Ä¢	Polynomial kernel: K(x_i, x_j) = (x_i^\top x_j + c)^d
(captures polynomial feature interactions).
	‚Ä¢	Radial Basis Function (RBF): K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)
(produces flexible, smooth boundaries that adapt locally).

‚∏ª

**Training logic**

Training involves solving a quadratic optimization problem using algorithms like Sequential Minimal Optimization (SMO).
Because only a subset of data points (the support vectors) define the decision boundary, the model focuses computational effort on the most informative samples.

Choosing the right kernel and hyperparameters is critical ‚Äî especially the RBF parameter \gamma and the regularization constant C.
Too large a \gamma leads to overfitting (narrow bumps around data), while too small a \gamma oversmooths the boundary.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	The classes are separable in some higher-dimensional space induced by the chosen kernel.
	‚Ä¢	Features are properly scaled (kernels are sensitive to magnitude).

Limitations
	‚Ä¢	Training can be computationally expensive for large datasets (O(n¬≤) memory).
	‚Ä¢	Hard to interpret ‚Äî the model becomes a black box of support vectors.
	‚Ä¢	Sensitive to hyperparameter choice (C, Œ≥, and kernel type).

Despite these challenges, kernel SVMs remain a gold standard for medium-sized structured data.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	C: regularization constant controlling margin width vs. misclassification tolerance.
	‚Ä¢	kernel: type of similarity function (‚Äúlinear‚Äù, ‚Äúpoly‚Äù, ‚Äúrbf‚Äù).
	‚Ä¢	Œ≥ (gamma): RBF width ‚Äî small values yield smoother boundaries, large values yield tighter fits.
	‚Ä¢	degree (d): degree of the polynomial kernel.
	‚Ä¢	coef0 (c): additive constant in the polynomial kernel, affecting curvature.

These parameters together shape the geometry of the decision surface.

‚∏ª

**Evaluation focus**

The model outputs raw decision scores, not probabilities.

Thus, evaluate with:

‚Ä¢	ROC-AUC and PR-AUC for ranking.
‚Ä¢	Accuracy, precision, and recall for classification thresholds.
‚Ä¢	Apply probability calibration (Platt scaling or isotonic regression) when calibrated outputs are needed.

Visualization of the decision surface can also reveal under- or overfitting patterns ‚Äî particularly useful in 2-D projections.

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	The relationship between inputs and outputs is non-linear but structured.
	‚Ä¢	The dataset size is moderate (up to a few tens of thousands).
	‚Ä¢	You need strong generalization without large models like neural nets.

Avoid it when:
	‚Ä¢	Data are huge (training cost grows quadratically).
	‚Ä¢	Features are extremely sparse (linear SVM may perform just as well).
	‚Ä¢	Interpretability or probability estimation is required.

‚∏ª

**References**

**Canonical papers**

1.	Boser, B., Guyon, I., & Vapnik, V. (1992). A Training Algorithm for Optimal Margin Classifiers. COLT.
2.	Scholkopf, B., & Smola, A. J. (2002). Learning with Kernels. MIT Press.
3.	Cristianini, N., & Shawe-Taylor, J. (2000). An Introduction to Support Vector Machines. Cambridge University Press.

**Web resources**

‚Ä¢	Scikit-learn User Guide ‚Äî SVM Kernels
	https://scikit-learn.org/stable/modules/svm.html#kernel-functionsÔøº
‚Ä¢	StatQuest ‚Äî The Kernel Trick Explained Clearly
	https://www.youtube.com/watch?v=Qc5IyLW_hnsÔøº

--------

Kernel SVMs represent the culmination of the margin-based approach ‚Äî transforming linear geometry into flexible, curved separations.
However, they still rely on pairwise comparisons between samples, making them computationally heavy for massive datasets.
The next family, Instance-based Models, turns this idea inside out: instead of learning a boundary, we classify by comparing new samples directly to past examples.

--------

### C. Instance-based Models

The models we have explored so far ‚Äî linear, probabilistic, and margin-based ‚Äî all build explicit decision functions.
They learn a set of parameters that define a boundary or rule, summarizing what they have seen.
Once trained, these models can forget the original data and rely solely on their learned representation to classify new points.

Instance-based models take the opposite approach.
They do not compress knowledge into coefficients or support vectors ‚Äî they remember.
They store the training data itself and classify new observations by comparing them directly to past examples.
In this sense, they learn not by abstraction, but by analogy.

‚ÄúTell me who your neighbors are, and I‚Äôll tell you who you are.‚Äù

This is the philosophy behind algorithms such as k-Nearest Neighbors (kNN) and related distance-based methods.

‚∏ª

**From Boundaries to Proximity**

While margin-based algorithms define decision boundaries globally, instance-based models make decisions locally.
Each new point is evaluated in the context of its surroundings.
If most of its neighbors belong to class A, the point is classified as A.
There is no need to assume linearity, normality, or even continuity ‚Äî the data itself defines the geometry.

This approach provides flexibility:
	‚Ä¢	It adapts naturally to non-linear and irregular shapes.
	‚Ä¢	It can represent complex interactions without explicit feature transformations.
	‚Ä¢	It works well even when only small regions of the data exhibit structure.

However, that flexibility comes at a cost:
storing and comparing all instances means high computational cost, and because it learns no global model, it is sensitive to noise and irrelevant features.

‚∏ª

**Why Instance-based Thinking Matters**

In many real-world problems, decisions are inherently local.
A rare medical case may resemble only a few other patients in a dataset.
An unusual image may be best understood by comparing it to its closest matches.
In these cases, global models can oversimplify patterns that only appear in small neighborhoods.

Instance-based methods capture this granularity.
They do not generalize through equations, but through similarity ‚Äî
an idea that also underlies modern deep learning embeddings and retrieval-based systems.

‚∏ª

**Conceptual Shift**

This family of models represents another philosophical turn:
	‚Ä¢	From probability (family A) to geometry (family B),
	‚Ä¢	and now to memory and proximity (family C).

Instead of asking ‚ÄúWhat is the separating function?‚Äù, we now ask

‚Äú**Which points are most similar to this one, and what can they tell me?**‚Äù

The heart of this method lies in the choice of distance metric ‚Äî
how we define ‚Äúsimilarity‚Äù between observations ‚Äî
and in how we decide how many neighbors (k) to consider.

These questions define the core of the next model,
the simplest and most iconic of this family:
k-Nearest Neighbors (kNN) ‚Äî a classifier that makes decisions by remembering and comparing, not by generalizing.

#### 9. Distance metrics, scaling, k selection.  (Distance metrics, scaling, k selection.)

**What is it?**

The k-Nearest Neighbors (kNN) algorithm is one of the simplest and most intuitive methods in machine learning.
It classifies a new observation based on the majority class among its k closest points in the training data.
Rather than learning a parametric function, kNN is a memory-based learner ‚Äî it stores all training samples and defers decision-making until prediction time.

First introduced by Fix and Hodges (1951) and later popularized by Cover and Hart (1967), kNN remains a foundational model for understanding the concept of similarity-based learning.

‚∏ª

**Why use it?**

kNN excels in situations where the relationship between features and labels is highly non-linear or difficult to model explicitly.
It adapts naturally to data geometry without assuming any functional form or distribution.

Key advantages:
	‚Ä¢	Non-parametric flexibility: learns directly from the data, no training optimization required.
	‚Ä¢	Intuitive interpretability: decisions depend on observable neighbors.
	‚Ä¢	Strong local adaptation: effective when nearby points share the same label.

Typical use cases include pattern recognition, anomaly detection, recommendation systems, and small tabular or image datasets.

‚∏ª

**Intuition**

Classification in kNN is based entirely on proximity:
points close in feature space are expected to belong to the same class.

For a new sample x_q:
	1.	Compute the distance between x_q and every training sample.
	2.	Select the k nearest points.
	3.	Assign the class most frequent among those neighbors (majority vote).

Formally, the decision rule can be written as:

$$
\hat{y}(x_q) = \text{mode}\big({y_i : x_i \in N_k(x_q)}\big)
$$

where N_k(x_q) is the set of the k nearest neighbors of x_q.

‚∏ª

**Mathematical foundation**

Distance is the essence of kNN.
Common distance functions include:

Euclidean distance
$$
d(x_i, x_j) = \sqrt{\sum_{m=1}^{p}(x_{im} - x_{jm})^2}
$$

Manhattan distance
$$
d(x_i, x_j) = \sum_{m=1}^{p} |x_{im} - x_{jm}|
$$

Minkowski distance (general form)
$$
d(x_i, x_j) = \left(\sum_{m=1}^{p} |x_{im} - x_{jm}|^r \right)^{1/r}
$$

Choosing the right distance depends on the scale and nature of the features.
All features must be normalized or standardized ‚Äî otherwise, attributes with large numeric ranges dominate the distance computation.

‚∏ª

**Training logic**

There is no explicit training phase.
The model stores all data points and performs computation only during inference.
When a new sample arrives, it calculates distances to all stored examples and applies the majority voting rule.

This makes kNN computationally light to train but heavy to predict, especially for large datasets.
Approximate nearest-neighbor methods (e.g., KD-Trees, Ball-Trees, FAISS) mitigate this limitation in practice.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	Samples close in feature space share the same label.
	‚Ä¢	All features contribute equally to distance (hence the need for scaling).

Limitations
	‚Ä¢	Computationally expensive for large datasets (O(n) per prediction).
	‚Ä¢	Sensitive to irrelevant features and the curse of dimensionality.
	‚Ä¢	Memory-intensive ‚Äî must store the full dataset.
	‚Ä¢	No direct probability output (only majority voting).

Despite these challenges, kNN provides a strong baseline for similarity-based reasoning.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	k (number of neighbors):
Controls smoothness of the decision boundary.
Small k ‚Üí flexible, but sensitive to noise.
Large k ‚Üí stable, but can blur class boundaries.
	‚Ä¢	metric:
Defines the notion of distance ‚Äî common choices include euclidean, manhattan, minkowski, or cosine.
	‚Ä¢	weights:
Determines voting scheme.
Uniform assigns equal weight to all neighbors; distance gives closer points more influence.
	‚Ä¢	scaling method:
Standardization or normalization to ensure comparable feature magnitudes.

‚∏ª

**Evaluation focus**

Because kNN decisions depend on local density and class balance, evaluation should consider:
	‚Ä¢	Accuracy and F1-score for balanced datasets.
	‚Ä¢	ROC-AUC and PR-AUC if calibrated scores or distances are used.
	‚Ä¢	Cross-validation to tune k ‚Äî typically using grid search on validation folds.

Visualization of decision boundaries is also valuable for diagnosing overfitting (very jagged regions indicate too small k).

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	The dataset is small or moderate in size.
	‚Ä¢	Relationships are complex but locally smooth.
	‚Ä¢	Interpretability and simplicity are preferred over training efficiency.

Avoid it when:
	‚Ä¢	Data are high-dimensional (distance loses meaning).
	‚Ä¢	Memory or inference speed is critical.
	‚Ä¢	Many irrelevant or categorical features dominate the dataset.

‚∏ª

**References**

Canonical papers

1.	Fix, E., & Hodges, J. L. (1951). Discriminatory Analysis, Nonparametric Discrimination: Consistency Properties. USAF School of Aviation Medicine.
2.	Cover, T., & Hart, P. (1967). Nearest Neighbor Pattern Classification. IEEE Transactions on Information Theory.
3.	Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning. Springer.

Web resources

‚Ä¢	Scikit-learn User Guide ‚Äî Nearest Neighbors
	https://scikit-learn.org/stable/modules/neighbors.htmlÔøº
‚Ä¢	StatQuest ‚Äî k-Nearest Neighbors Explained Clearly
https://www.youtube.com/watch?v=HVXime0nQeIÔøº


-------

k-Nearest Neighbors teaches us that classification can succeed without an explicit model ‚Äî simply by remembering and comparing.
Yet, this flexibility comes with computational cost and sensitivity to noise.
To overcome these issues, the next family of algorithms introduces hierarchical decision structures that partition the feature space efficiently and transparently.

-------

### D. Tree-based Models

The families of models we‚Äôve seen so far ‚Äî linear, margin-based, and instance-based ‚Äî each interpret learning through a different lens:
linearity, geometry, or similarity.
Tree-based models introduce yet another way of thinking: learning through hierarchical decisions.

Instead of computing distances or probabilities, a tree classifier asks questions about the data ‚Äî one at a time ‚Äî and follows the path that best separates the classes.
At each node, the model evaluates a simple rule such as

‚ÄúIs the feature x‚ÇÅ greater than 3.5?‚Äù
and based on the answer, it splits the data into more homogeneous groups.

By repeating this process recursively, the model builds a decision structure, not unlike how humans reason through choices:
first broad distinctions, then finer refinements.

‚∏ª

**From Flat Geometry to Hierarchical Reasoning**

Linear and margin-based models see the world as continuous surfaces ‚Äî boundaries drawn across the entire feature space.
Tree-based models, in contrast, partition that space into discrete, interpretable regions.
Each leaf of the tree represents a decision rule derived directly from the data.

For example:
	‚Ä¢	If income > 50,000 ‚Üí branch A
	‚Ä¢	Else ‚Üí branch B
Then within branch A, if age < 35 ‚Üí class = ‚Äúapprove loan‚Äù
Else ‚Üí class = ‚Äúdeny loan‚Äù

This divide-and-conquer approach allows trees to capture non-linear, non-parametric relationships with exceptional interpretability.

‚∏ª

**Why Tree-based Models Matter**

Tree algorithms provide a natural balance between flexibility and interpretability.
They adapt to almost any data distribution without requiring scaling or transformations, yet their internal logic can be visualized and explained to non-technical audiences.

Key advantages include:
	‚Ä¢	Interpretability: the structure itself is the model ‚Äî easy to visualize and audit.
	‚Ä¢	Non-linearity: decision boundaries can be irregular and data-driven.
	‚Ä¢	Feature selection: the tree automatically identifies informative variables.
	‚Ä¢	Handling of mixed data: numeric and categorical features coexist seamlessly.

However, single trees are also fragile.
They can easily overfit, memorizing noise instead of general patterns.
This limitation led to the rise of ensemble methods (discussed in the next family), which combine multiple trees to enhance stability and accuracy.

‚∏ª

**Philosophical Shift**

Tree-based models represent a transition from global reasoning to hierarchical segmentation.
They learn not by summarizing all data at once, but by iteratively refining structure ‚Äî dividing the feature space into progressively purer subsets.
Each split increases local homogeneity, guided by impurity measures such as Gini index or entropy.

In essence, while kNN looks outward to find similarity,
a decision tree looks inward, carving structure from within.

‚∏ª

**What‚Äôs Next**

In this family, we‚Äôll explore two key formulations:
	1.	Decision Trees (CART formulation):
The classic recursive partitioning model that builds binary splits to minimize impurity.
	2.	Cost-sensitive Trees (class weighting and impurity adjustments):
An adaptation of decision trees for imbalanced datasets and differential misclassification costs,
extending fairness and robustness in practical applications.

#### 10. Decision Trees (CART)

What is it?

A Decision Tree is a non-parametric, hierarchical model that predicts outcomes by recursively splitting data into subsets based on feature values.
Each internal node represents a question about an input feature (e.g., Is income > 50,000?), each branch represents an answer (Yes/No), and each leaf node corresponds to a final decision or class label.

The most common algorithm for constructing classification trees is CART (Classification and Regression Trees), introduced by Breiman, Friedman, Olshen, and Stone (1984).
CART builds binary trees that maximize class purity at every split, forming a transparent, interpretable decision structure.

‚∏ª

Why use it?

Decision Trees are widely used because they mimic human decision-making.
Their logic is straightforward, their boundaries are non-linear, and their predictions can be visualized and explained directly.

They work especially well when:
	‚Ä¢	The relationship between features and labels is non-linear or complex.
	‚Ä¢	Interpretability and auditability are important (e.g., finance, healthcare, public policy).
	‚Ä¢	Data include both numerical and categorical variables.
	‚Ä¢	Feature interactions exist but are difficult to specify manually.

Advantages:
	‚Ä¢	No need for feature scaling or normalization.
	‚Ä¢	Naturally handle missing values.
	‚Ä¢	Easy to visualize and explain.

‚∏ª

Intuition

Decision Trees learn by dividing the feature space into increasingly homogeneous regions.
At each node, the algorithm selects the feature and threshold that best split the data to reduce impurity.

Imagine trying to separate apples from oranges by asking a sequence of yes/no questions:

‚ÄúIs color > 0.5 on the red channel?‚Äù
‚ÄúIs diameter > 7 cm?‚Äù

Each question filters the data into smaller, purer subsets until each group contains mostly one class.

The resulting structure can be visualized as a tree:
	‚Ä¢	Root node: all samples.
	‚Ä¢	Internal nodes: decision rules.
	‚Ä¢	Leaves: final class predictions.

‚∏ª

Mathematical foundation

The central concept is impurity minimization ‚Äî choosing splits that make child nodes as pure as possible.

Common impurity measures:

Gini Index

$$
G(t) = 1 - \sum_{k=1}^{K} p_{k,t}^2
$$

Entropy (Information Gain)

$$
H(t) = - \sum_{k=1}^{K} p_{k,t} \log_2(p_{k,t})
$$

At each node t, the algorithm evaluates all possible splits and selects the feature j and threshold s that minimize the weighted impurity of the child nodes:

$$
\text{Split}(j, s) = \arg\min_{j,s} \Big[ \frac{N_L}{N} I(L) + \frac{N_R}{N} I(R) \Big]
$$

where I(\cdot) represents the impurity function (Gini or entropy), and N_L, N_R are the sample counts in left and right branches.

‚∏ª

**Training logic**

1.	Start with the full dataset as the root node.
2.	Evaluate every possible feature and threshold to find the split that minimizes impurity.
3.	Partition the data into left and right child nodes.
4.	Repeat recursively for each branch until:
‚Ä¢	A stopping criterion is reached (e.g., max_depth, min_samples_leaf).
‚Ä¢	The node is pure (all samples belong to the same class).
‚Ä¢	No further improvement in impurity is possible.
	5.	Assign the most frequent class to each terminal node.

This recursive partitioning yields a tree structure that can perfectly fit training data ‚Äî which is why pruning is essential to prevent overfitting.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	Data can be effectively separated by axis-aligned splits.
	‚Ä¢	Each feature contributes independently to the decision process.

Limitations
	‚Ä¢	Highly prone to overfitting without pruning or constraints.
	‚Ä¢	Small changes in data can cause large structural variations (instability).
	‚Ä¢	Poor extrapolation beyond training data.
	‚Ä¢	Prefers features with more levels (bias toward continuous variables).

Despite these drawbacks, trees remain the foundation for more advanced ensemble methods (e.g., Random Forest, XGBoost).

‚∏ª

**Key hyperparameters (conceptual view)**

‚Ä¢	max_depth: maximum depth of the tree (controls overfitting).
‚Ä¢	min_samples_split / min_samples_leaf: minimum number of samples required to split or form a leaf.
‚Ä¢	criterion: impurity measure (gini or entropy).
‚Ä¢	class_weight: adjusts importance of classes (especially for imbalance).
‚Ä¢	max_features: number of features to consider when looking for the best split.

These parameters control the balance between model complexity and generalization.

‚∏ª

**Evaluation focus**

Since Decision Trees can overfit easily, evaluation should emphasize generalization performance:
	‚Ä¢	Use cross-validation or pruning to avoid overly deep trees.
	‚Ä¢	Assess accuracy, F1-score, or ROC-AUC, depending on class balance.
	‚Ä¢	Analyze feature importance ‚Äî derived from impurity reduction ‚Äî to interpret the model‚Äôs reasoning.

Visualizing the tree structure (via plot_tree or graphviz) also helps validate interpretability and logic.

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	Transparency and interpretability are required.
	‚Ä¢	Data relationships are highly non-linear.
	‚Ä¢	There are mixed or missing feature types.

Avoid it when:
	‚Ä¢	Dataset is large and noisy (prefer ensembles).
	‚Ä¢	Decision boundaries are smooth or continuous (SVMs or neural nets may perform better).
	‚Ä¢	Stability and reproducibility are critical (trees are sensitive to small perturbations).

‚∏ª

**References**

Canonical papers

1.	Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and Regression Trees. Wadsworth.
2.	Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.
3.	Loh, W.-Y. (2011). Classification and Regression Trees. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery.

Web resources

‚Ä¢	Scikit-learn User Guide ‚Äî Decision Trees

	https://scikit-learn.org/stable/modules/tree.htmlÔøº

	
‚Ä¢	StatQuest ‚Äî Decision Trees Clearly Explained

	https://www.youtube.com/watch?v=7VeUPuFGJHkÔøº



------

Decision Trees bring structure and transparency to classification.
However, when datasets are imbalanced or certain errors are more costly than others, treating all splits equally becomes inefficient or even unfair.
The next variant ‚Äî Cost-sensitive Trees ‚Äî adapts the same hierarchical framework to account for class imbalance and misclassification costs,
making decisions not only accurate but also equitable.

------

#### 11. Cost-sensitive Trees (class weights, impurity adjustments)

**What is it?**

Cost-sensitive Decision Trees extend the standard CART framework to handle imbalanced datasets or unequal misclassification costs.
While traditional Decision Trees minimize overall impurity as if all errors were equally costly, cost-sensitive variants introduce weighting schemes that prioritize critical classes or reduce bias toward majority outcomes.

This adaptation ensures that the model does not simply aim for accuracy but optimizes for risk-adjusted correctness ‚Äî a crucial distinction in domains such as fraud detection, medical diagnosis, and credit scoring, where false negatives and false positives carry very different consequences.

‚∏ª

**Why use it?**

In many real-world problems, accuracy alone is misleading.
A model predicting ‚Äúno fraud‚Äù for every transaction might achieve 99.9% accuracy if frauds are rare ‚Äî yet be useless in practice.
Cost-sensitive trees address this by incorporating error cost asymmetry directly into the learning process.

They are especially valuable when:
	‚Ä¢	One class is rare but important (e.g., fraud, disease, failure).
	‚Ä¢	The cost of false negatives ‚â† false positives.
	‚Ä¢	Regulatory or ethical contexts demand fair treatment of minority cases.

Rather than discarding imbalance handling to post-processing (e.g., resampling), these trees embed fairness and balance into the tree‚Äôs growth criteria.

‚∏ª

**Intuition**

In standard trees, impurity measures (like Gini or entropy) assume each class contributes equally.
Cost-sensitive trees modify these measures by weighting observations or classes according to their importance.

During training, a split that correctly classifies a rare but important class is given more credit, while misclassifying it incurs greater penalty.
This shifts the tree‚Äôs growth toward decisions that protect against costly mistakes.

Conceptually, instead of asking

‚ÄúWhich split reduces impurity the most?‚Äù
the model asks
‚ÄúWhich split reduces weighted impurity ‚Äî given how important each class is?‚Äù

‚∏ª

**Mathematical foundation**

Let w_k represent the weight (or cost) associated with class k.
The weighted Gini impurity at a node t becomes:

$$
G_w(t) = 1 - \sum_{k=1}^{K} \left( \frac{w_k p_{k,t}}{\sum_{j=1}^{K} w_j p_{j,t}} \right)^2
$$

Similarly, the weighted entropy formulation is:

$$
H_w(t) = - \sum_{k=1}^{K} w_k p_{k,t} \log_2(p_{k,t})
$$

The split criterion generalizes to:

$$
\text{Split}(j, s) = \arg\min_{j,s} \Big[ \frac{N_L}{N} I_w(L) + \frac{N_R}{N} I_w(R) \Big]
$$

where I_w(\cdot) represents the weighted impurity measure.
Here, w_k can reflect class imbalance, monetary cost, or policy-driven penalties.

Finally, prediction at each leaf node uses weighted majority voting, not raw frequency:

$$
\hat{y}(x) = \arg\max_{k} \left( w_k , p_{k,\text{leaf}} \right)
$$

‚∏ª

**Training logic**

Training follows the same recursive partitioning steps as CART but introduces weight-adjusted impurity calculations:
	1.	Compute class distributions and weights at each node.
	2.	Evaluate all candidate splits based on weighted impurity.
	3.	Select the split that minimizes weighted impurity loss.
	4.	Recurse until stopping criteria are met.

Optionally, misclassification cost matrices can be defined explicitly, assigning higher penalties to specific errors ‚Äî for example:

Cost(false negative) = 10 √ó Cost(false positive)

This ensures that the tree structure aligns with the application‚Äôs true risk profile.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	Class imbalance or differential misclassification costs are known or estimable.
	‚Ä¢	Assigned weights reasonably approximate the real-world importance of errors.

Limitations
	‚Ä¢	Sensitive to incorrect or arbitrary weight assignment.
	‚Ä¢	May still overfit if imbalance is extreme.
	‚Ä¢	Weighted impurities can bias splits toward small subgroups if not carefully regularized.
	‚Ä¢	Interpretability slightly decreases when costs are implicit or application-specific.

Still, in regulated or high-stakes domains, these trees provide more responsible and realistic decision boundaries.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	class_weight: assigns relative importance to each class (balanced, custom dictionary).
	‚Ä¢	sample_weight: provides per-observation control during training.
	‚Ä¢	criterion: impurity measure (gini, entropy, or weighted variants).
	‚Ä¢	min_samples_split / min_samples_leaf: constrain growth to prevent overfitting on minority subsets.
	‚Ä¢	max_depth: limits complexity, improving generalization on skewed data.

These parameters shape how the model distributes focus between majority stability and minority sensitivity.

‚∏ª

**Evaluation focus**

Accuracy is insufficient ‚Äî evaluation must prioritize cost-aware metrics:
	‚Ä¢	Precision, Recall, and F1-score, especially per class.
	‚Ä¢	ROC‚ÄìAUC and PR‚ÄìAUC for discrimination quality.
	‚Ä¢	Confusion matrix weighted by cost to visualize trade-offs.
	‚Ä¢	Balanced accuracy or Matthews Correlation Coefficient (MCC) for overall fairness.

Cross-validation should preserve class ratios (stratified folds) to avoid misleading validation results.

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	Data are imbalanced and certain misclassifications are more serious.
	‚Ä¢	You can estimate or define realistic cost ratios.
	‚Ä¢	Fairness, ethics, or policy compliance require equitable treatment.

Avoid it when:
	‚Ä¢	Costs are unknown or arbitrary.
	‚Ä¢	Data imbalance is minor (standard CART suffices).
	‚Ä¢	Simpler rebalancing techniques (SMOTE, class weights) already yield acceptable results.

‚∏ª

**References**

Canonical papers

1.	Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and Regression Trees. Wadsworth.
2.	Elkan, C. (2001). The Foundations of Cost-Sensitive Learning. IJCAI.
3.	Ling, C. X., & Sheng, V. S. (2008). Cost-Sensitive Learning and the Class Imbalance Problem. Springer.

Web resources

‚Ä¢	Scikit-learn User Guide ‚Äî Class and Sample Weights
https://scikit-learn.org/stable/modules/tree.html#class-weightÔøº

‚Ä¢	Medium ‚Äî Understanding Cost-Sensitive Learning in Decision Trees
https://medium.com/@dataman-in-ai/cost-sensitive-decision-trees-8faae4b4f40cÔøº


-----

Cost-sensitive Decision Trees remind us that accuracy without context can be deceptive.
They embed ethical and economic considerations into the decision process ‚Äî a precursor to responsible AI.
Yet, even with weighting and pruning, single trees remain unstable and limited in expressiveness.
The next great leap in classification came from ensembles, which combine the wisdom of many trees to achieve remarkable robustness and precision.
Next: E. Ensemble Models ‚Äî Learning by Aggregation.

-----

### E. Ensemble Models - learning by Aggregation.

**From Individual Learners to Collective Intelligence**

Up to this point, each model we explored ‚Äî linear, geometric, instance-based, or tree-based ‚Äî made predictions independently.
They learned a single decision function, a solitary view of the data.
However, even the best single model is prone to bias, noise, or variance.
The idea behind ensemble learning is both simple and revolutionary:

‚ÄúInstead of relying on one imperfect model, let‚Äôs combine many of them ‚Äî
and let their collective wisdom yield better, more stable predictions.‚Äù

This is the core philosophy of ensemble methods: learning by aggregation.

‚∏ª

**The Core Intuition**

Imagine asking several experts to classify an image, predict a disease, or decide on a loan.
Each will have their own perspective, error tendencies, and biases.
But if their opinions are aggregated intelligently ‚Äî by averaging, voting, or weighting ‚Äî
the final decision often surpasses that of any individual expert.

In statistical terms, ensemble methods reduce variance and bias by combining multiple weak or moderately strong learners into a single robust predictor.
Their success relies on a fundamental principle of collective intelligence:

‚ÄúDiversity + Independence + Aggregation = Accuracy.‚Äù

When models make uncorrelated errors, their ensemble tends to cancel noise while reinforcing the true signal.

‚∏ª

**Why Ensembles Work**

1.	Variance Reduction (Stability)
Averaging many unstable models (like trees) smooths out noise and improves generalization ‚Äî the essence of Bagging and Random Forests.

2.	Bias Reduction (Precision)
Sequentially combining weak learners where each corrects the previous one‚Äôs mistakes ‚Äî the logic behind Boosting algorithms (AdaBoost, XGBoost, etc.) ‚Äî drives models toward higher accuracy.

3.	Feature Interaction and Complexity
Ensembles automatically model complex relationships by aggregating multiple decision paths, without requiring explicit feature engineering.

4.	Universal Adaptability
Any base model can be ensembled ‚Äî decision trees, linear classifiers, neural nets ‚Äî making the framework extremely flexible.

‚∏ª

**A New Philosophy of Learning**

Ensemble learning reflects a shift from ‚Äúone best model‚Äù to ‚Äúmany cooperating models‚Äù.
It embodies a pragmatic view of intelligence: error is inevitable, but collective reasoning minimizes it.

Each ensemble technique differs mainly in how it builds and aggregates its members:
	‚Ä¢	Some train learners independently and combine them later (e.g., Bagging, Random Forests, Extra Trees).
	‚Ä¢	Others train them sequentially, each focusing on correcting prior errors (e.g., AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost).

This diversity of construction gives rise to two main paradigms:
	‚Ä¢	Parallel ensembles ‚Äî focus on stability.
	‚Ä¢	Sequential ensembles ‚Äî focus on precision.

‚∏ª

**Why They Dominate Modern ML**

In applied machine learning ‚Äî especially with tabular data ‚Äî ensemble models have become the gold standard.
They consistently outperform deep neural networks on structured datasets due to their ability to:
	‚Ä¢	Handle missing data gracefully.
	‚Ä¢	Capture complex feature interactions automatically.
	‚Ä¢	Scale efficiently on large datasets.
	‚Ä¢	Require minimal preprocessing or tuning to achieve competitive results.

Their performance, interpretability (through feature importance), and ease of deployment make them central to both academic and industrial ML pipelines.

‚∏ª

**Conceptual Transition from Trees to Forests**

Tree-based models (as seen in the previous section) are like individual decision-makers: interpretable but unstable.
Ensembles transform them into communities of decision-makers ‚Äî each tree exploring slightly different perspectives of the same problem.

By aggregating hundreds or thousands of shallow trees, the ensemble stabilizes predictions while retaining nonlinearity and interpretability.
This philosophy underlies nearly all modern classifiers used in data science competitions, enterprise systems, and research pipelines.

‚∏ª

**What‚Äôs Next**

We will explore the most influential ensemble methods, grouped by their aggregation logic:
	1.	Bagging (Bootstrap Aggregating) ‚Äî parallel model averaging for variance reduction.
	2.	Random Forests ‚Äî ensemble of de-correlated trees for balanced performance.
	3.	Extra Trees ‚Äî extreme randomization for even lower variance.
	4.	AdaBoost ‚Äî sequentially boosted weak learners focusing on hard examples.
	5.	Gradient Boosting (GBDT) ‚Äî gradient-based sequential optimization.
	6.	XGBoost ‚Äî regularized, optimized GBDT implementation.
	7.	LightGBM ‚Äî efficient, leaf-wise gradient boosting for speed and scalability.
	8.	CatBoost ‚Äî boosting with categorical encoding and order-based regularization.

Each of these methods represents a refinement of the ensemble idea ‚Äî
from statistical aggregation to algorithmic synergy ‚Äî
and together they form the backbone of modern supervised learning.



#### 12. Bagging (Bootstrap Aggregating)

**What is it?**

Bagging, short for Bootstrap Aggregating, is one of the simplest yet most powerful ensemble methods in machine learning.
Proposed by Leo Breiman (1996), it combines multiple models trained on different random subsets of the same dataset and averages their predictions to reduce variance and improve stability.

Each model (often a decision tree) learns from a slightly different perspective of the data, thanks to bootstrap sampling ‚Äî random sampling with replacement.
By aggregating their outputs, Bagging creates a smoother, more robust prediction than any single model could achieve.

‚∏ª

**Why use it?**

Bagging‚Äôs main strength lies in variance reduction.
Many models ‚Äî particularly high-variance learners like decision trees ‚Äî tend to overfit training data.
Bagging combats this by averaging multiple overfit models so that their random errors cancel out while the true signal remains.

It is especially effective when:
	‚Ä¢	The base learner is unstable (small data changes ‚Üí large model changes).
	‚Ä¢	The dataset has moderate noise or complex non-linear relationships.
	‚Ä¢	The goal is to improve generalization without increasing bias.

Common applications include:
	‚Ä¢	Classification tasks with noisy or heterogeneous data.
	‚Ä¢	Risk prediction in finance and healthcare.
	‚Ä¢	Foundation for modern ensemble methods (e.g., Random Forest).

‚∏ª

**Intuition**

Imagine teaching several students the same topic, but each with slightly different subsets of examples.
Individually, their conclusions vary.
But if we take the average of their answers, we often get closer to the truth.

That‚Äôs Bagging in essence ‚Äî a democratic system of weak learners voting to produce a stable result.

In formal terms, Bagging constructs multiple bootstrap replicas of the dataset:

$$
D^{(b)} = { (x_i, y_i) }_{i=1}^{n_b}, \quad \text{where each sample is drawn with replacement from } D
$$

For each bootstrap dataset D^{(b)}, a base model f^{(b)}(x) is trained.
At prediction time, Bagging aggregates their outputs:

For classification (majority voting):

$$
\hat{y}(x) = \text{mode}{ f^{(1)}(x), f^{(2)}(x), \dots, f^{(B)}(x) }
$$

For regression (averaging):

$$
\hat{y}(x) = \frac{1}{B} \sum_{b=1}^{B} f^{(b)}(x)
$$

‚∏ª

**Mathematical foundation**

Let the true model be f(x), and each base estimator f^{(b)}(x) have bias \text{Bias}(f^{(b)}) and variance \text{Var}(f^{(b)}).

Averaging reduces the variance term approximately as:

$$
\text{Var}\big(\hat{f}_{\text{bag}}(x)\big) \approx \frac{1}{B} \text{Var}(f^{(b)}(x))
$$

assuming models are uncorrelated.
In practice, Bagging works because the random sampling makes models only partially correlated, which still leads to significant variance reduction.

The overall mean squared error (MSE) decomposition illustrates this effect:

$$
\text{MSE} = \text{Bias}^2 + \text{Var} + \text{Noise}
$$

Bagging leaves bias mostly unchanged but substantially decreases the variance component, improving predictive performance.

‚∏ª

**Training logic**

1.	Bootstrap sampling:
Draw B random datasets from the original data, each of size n, sampling with replacement.

2.	Train base learners:
Fit one model f^{(b)} on each bootstrap sample independently.

3.	Aggregate predictions:
‚Ä¢	For regression ‚Üí take the average.
‚Ä¢	For classification ‚Üí take the majority vote.

4.	(Optional) Out-of-Bag (OOB) estimation:
Since about 37% of data are left out of each bootstrap sample, Bagging can estimate its own test error using those unseen samples ‚Äî no need for a separate validation set.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	The base learner has high variance and benefits from averaging (e.g., decision trees).
	‚Ä¢	Samples are independent and identically distributed (i.i.d.).

Limitations
	‚Ä¢	Ineffective for low-variance, high-bias models (e.g., linear models).
	‚Ä¢	Aggregation reduces interpretability ‚Äî the ensemble becomes opaque.
	‚Ä¢	Computationally heavier (many models trained in parallel).

Bagging is less about sophistication and more about stability through redundancy.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	n_estimators: number of models in the ensemble (B).
	‚Ä¢	max_samples: fraction or number of samples drawn per bootstrap.
	‚Ä¢	max_features: number of features considered when training each model.
	‚Ä¢	bootstrap: whether to sample with replacement (True = Bagging).
	‚Ä¢	oob_score: whether to estimate generalization error using out-of-bag samples.

These parameters control the trade-off between diversity and computational cost.

‚∏ª

**Evaluation focus**

Bagging improves variance-driven metrics, such as:
	‚Ä¢	Accuracy or ROC‚ÄìAUC on noisy datasets.
	‚Ä¢	Stability across folds (lower variance in cross-validation).
	‚Ä¢	OOB score, a direct estimate of test error.

Inspecting feature importance (averaged across models) also helps explain ensemble decisions.

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	The base model is unstable (e.g., Decision Trees).
	‚Ä¢	Dataset is moderately noisy or small.
	‚Ä¢	You want a simple ensemble with strong variance reduction.

Avoid it when:
	‚Ä¢	The base model is already stable (e.g., linear regression).
	‚Ä¢	You need highly interpretable models.
	‚Ä¢	The dataset is extremely large and computation is constrained.

‚∏ª

**References**

Canonical papers

1.	Breiman, L. (1996). Bagging Predictors. Machine Learning, 24(2), 123‚Äì140.
2.	Opitz, D., & Maclin, R. (1999). Popular Ensemble Methods: An Empirical Study. Journal of Artificial Intelligence Research, 11, 169‚Äì198.
3.	Dietterich, T. (2000). Ensemble Methods in Machine Learning. Springer.

Web resources

‚Ä¢	Scikit-learn User Guide ‚Äî Bagging Classifier
https://scikit-learn.org/stable/modules/ensemble.html#baggingÔøº

‚Ä¢	StatQuest ‚Äî Bagging and Random Forests Explained
https://www.youtube.com/watch?v=nyxTdL_4Q-QÔøº


-----

Bagging introduced the principle of variance reduction through random resampling and aggregation.
However, all base models in Bagging are trained independently ‚Äî which means they might still explore redundant regions of the feature space.
The next logical evolution, Random Forests, refines this idea by injecting randomness not only in the data but also in the features ‚Äî
creating a forest of de-correlated trees that balance accuracy, robustness, and interpretability.

-----

#### Random Forest.

**What is it?**

Random Forests are one of the most widely used and successful classification algorithms ever developed.
Introduced by Leo Breiman (2001), they extend the Bagging idea by adding an additional layer of randomness ‚Äî not only do they sample the data (bootstrapping), but they also sample the features used to grow each tree.

Each tree in the forest learns from a slightly different subset of data and features, ensuring that the individual trees are decorrelated.
When these trees vote together, their errors tend to cancel out while their predictive signals reinforce one another.

This dual randomness ‚Äî in rows and columns ‚Äî is what makes Random Forests both robust and generalizable, even on complex datasets.

‚∏ª

**Why use it?**

Random Forests combine the interpretability of Decision Trees with the stability and predictive power of ensembles.
They are particularly effective when:
	‚Ä¢	Data are non-linear, noisy, or high-dimensional.
	‚Ä¢	You need strong performance with minimal tuning.
	‚Ä¢	You value feature importance and partial interpretability.
	‚Ä¢	The dataset mixes categorical and numerical variables.

Applications span nearly every domain: credit scoring, bioinformatics, text classification, remote sensing, fraud detection, and industrial quality control.

Their reliability and ease of use have made them a default baseline for structured data.

‚∏ª

**Intuition**

Bagging already showed that averaging multiple trees reduces variance.
However, if all trees see the same dominant features, they become highly correlated, and averaging provides limited benefit.

Random Forests solve this by introducing feature randomness.
At each split in every tree, only a random subset of features (of size m) is considered.
This simple change forces diversity among trees, creating an ensemble that explores different parts of the feature space.

Think of it as a committee where each member has access to different information ‚Äî their collective vote is more balanced and less biased by any single dominant factor.

‚∏ª

**Mathematical foundation**

For a training dataset D = \{(x_i, y_i)\}_{i=1}^{n}, Random Forests train B trees independently:

Each tree T_b is trained on a bootstrap sample D^{(b)}.
At each node split, a random subset of m features is drawn (from total p).
The split is chosen to minimize impurity:

$$
\text{Split}(j, s) = \arg\min_{j \in \text{Features}(m), s} \Big[ \frac{N_L}{N} I(L) + \frac{N_R}{N} I(R) \Big]
$$

After training, the final ensemble prediction is the majority vote (classification):

$$
\hat{y}(x) = \text{mode}{T_1(x), T_2(x), \dots, T_B(x)}
$$

or the average (regression):

$$
\hat{y}(x) = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
$$

The expected variance of the ensemble prediction reduces approximately as:

$$
\text{Var}(\hat{y}) = \rho , \text{Var}(T) + \frac{1 - \rho}{B} \text{Var}(T)
$$

where \rho is the average correlation between trees.
Reducing \rho ‚Äî through feature randomness ‚Äî is the key to Random Forest‚Äôs strength.

‚∏ª

**Training logic**

1.	Bootstrap sampling: draw multiple datasets with replacement.
2.	Tree construction: at each split, select a random subset of features (mtry).
3.	Grow trees fully (no pruning): this maximizes diversity among trees.
4.	Aggregation: combine tree outputs by voting or averaging.
5.	Out-of-Bag (OOB) error estimation: use the ~37% of samples left out of each bootstrap to measure generalization.

Random Forests grow hundreds or thousands of trees in parallel, each exploring a unique ‚Äúview‚Äù of the problem.

‚∏ª

**Assumptions and limitations**

Assumptions
	‚Ä¢	The signal can be captured through feature interactions and splits.
	‚Ä¢	Trees are uncorrelated enough for averaging to reduce variance.

Limitations
	‚Ä¢	Interpretability decreases as the number of trees grows.
	‚Ä¢	Predictions are slower with large forests.
	‚Ä¢	Feature importance may be biased toward variables with many categories or scales.
	‚Ä¢	Struggles slightly on extremely high-dimensional sparse data (where linear models shine).

Despite these, Random Forests remain one of the best general-purpose models in existence.

‚∏ª

**Key hyperparameters (conceptual view)**

‚Ä¢	n_estimators: number of trees in the forest (more trees ‚Üí lower variance).
‚Ä¢	max_features: number of features considered per split (controls correlation).
‚Ä¢	max_depth, min_samples_split, min_samples_leaf: limit overfitting.
‚Ä¢	bootstrap: whether sampling with replacement is used.
‚Ä¢	class_weight: handles imbalance by reweighting minority classes.
‚Ä¢	oob_score: estimates test error using out-of-bag samples.

These parameters jointly balance bias, variance, and correlation among trees.

‚∏ª

**Evaluation focus**

Random Forests should be evaluated for:
	‚Ä¢	Accuracy, F1-score, ROC‚ÄìAUC, and PR‚ÄìAUC.
	‚Ä¢	OOB score for internal validation.
	‚Ä¢	Feature importance (Gini importance or permutation importance).
	‚Ä¢	Stability across random seeds ‚Äî reliable models show minimal variance across runs.

They tend to perform exceptionally well on structured, tabular datasets with minimal preprocessing.

‚∏ª

**When to use / When not to use**

Use it when:
	‚Ä¢	The dataset has complex feature interactions.
	‚Ä¢	You need a strong baseline without much tuning.
	‚Ä¢	Data are noisy or moderately imbalanced.
	‚Ä¢	Interpretability via feature importance is sufficient.

Avoid it when:

‚Ä¢	You require real-time predictions with strict latency.
‚Ä¢	The dataset is extremely high-dimensional and sparse (consider linear or kernel methods).
‚Ä¢	You need transparent, human-interpretable rules.

‚∏ª

**References**

Canonical papers
	1.	Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5‚Äì32.
	2.	Liaw, A., & Wiener, M. (2002). Classification and Regression by randomForest. R News, 2(3), 18‚Äì22.
	3.	Biau, G., & Scornet, E. (2016). A Random Forest Guided Tour. TEST Journal, 25(2), 197‚Äì227.

Web resources
	‚Ä¢	Scikit-learn User Guide ‚Äî Random Forests
https://scikit-learn.org/stable/modules/ensemble.html#random-forestsÔøº
	‚Ä¢	StatQuest ‚Äî Random Forests Explained Clearly
https://www.youtube.com/watch?v=J4Wdy0Wc_xQÔøº


-----

Random Forests solved one of Bagging‚Äôs main challenges ‚Äî correlation between trees ‚Äî by introducing randomness at both the sample and feature levels.
This simple innovation transformed ensembles from an academic curiosity into an industrial workhorse.

Still, even decorrelated trees carry a certain inefficiency: each split searches deterministically for the best threshold, often leading to redundant partitions.
The next algorithm ‚Äî Extra Trees (Extremely Randomized Trees) ‚Äî pushes randomness further, drawing thresholds at random to achieve even greater diversity and variance reduction without increasing bias.

-----

#### Extra Trees.

What is it?

Extra Trees, short for Extremely Randomized Trees, extend the idea of Random Forests by injecting even more randomness into the tree-building process.
Proposed by Pierre Geurts, Damien Ernst, and Louis Wehenkel (2006), this method aims to further reduce model variance by increasing diversity among trees.

While Random Forests randomize both data samples (bootstrapping) and feature subsets, Extra Trees go a step further ‚Äî
they randomize the split thresholds themselves instead of searching for the optimal ones.

This deliberate randomization might sound counterintuitive, but it creates a stronger ensemble through diversity, often achieving accuracy similar to or better than Random Forests, with less computational cost.

‚∏ª

Why use it?

Extra Trees are particularly useful when:
	‚Ä¢	You want a fast and robust ensemble for large, high-dimensional datasets.
	‚Ä¢	The dataset contains noisy or redundant features.
	‚Ä¢	You need variance reduction without overfitting.

Because Extra Trees use the entire training set (no bootstrapping by default) and avoid exhaustive split searches, they are faster to train and sometimes generalize even better than Random Forests.

They are widely used in industrial and academic applications such as:
	‚Ä¢	Fraud detection and anomaly detection.
	‚Ä¢	Sensor-based fault prediction.
	‚Ä¢	Bioinformatics and genomics (large p, small n settings).

‚∏ª

Intuition

Random Forests already reduce variance through feature randomness, but each split still chooses the best threshold for splitting data.
Extra Trees add another layer of randomness by choosing both the feature and the split threshold randomly, without evaluating all possible cut points.

This has two main consequences:
	1.	Faster training, since the best split is not searched exhaustively.
	2.	Higher tree diversity, since trees differ even more in structure, reducing correlation and variance.

In practice, Extra Trees tend to have slightly higher bias than Random Forests but lower variance, leading to similar or improved overall performance.

‚∏ª

Mathematical foundation

At each node in a tree:
	1.	Randomly select a subset of features of size m.
	2.	For each selected feature x_j, draw a random split threshold s_j uniformly within its value range.
	3.	Choose one random pair (x_j, s_j) to perform the split.

Thus, the decision rule is defined as:

$$
\text{Split}(x) =
\begin{cases}
x_j \leq s_j & \text{send to left branch} \
x_j > s_j & \text{send to right branch}
\end{cases}
$$

Each tree is trained on the entire dataset (no bootstrapping) by default, and the ensemble prediction is obtained by averaging or majority voting:

$$
\hat{y}(x) = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
$$

This added randomness decorrelates the trees, improving the generalization of the aggregated model.

‚∏ª

Training logic
	1.	Sample generation (optional):
Unlike Bagging and Random Forests, Extra Trees often use the entire dataset for each tree.
	2.	Random feature selection:
At each node, select a random subset of features.
	3.	Random threshold selection:
Instead of computing the best split, draw a threshold uniformly at random for each chosen feature.
	4.	Recursive splitting:
Repeat until a stopping criterion is reached (max depth, min samples per leaf).
	5.	Aggregation:
Average or vote across all trees for the final prediction.

This randomness may appear naive, but the ensemble effect smooths individual imperfections into strong generalization.

‚∏ª

Assumptions and limitations

Assumptions
	‚Ä¢	The signal is complex but stable enough to tolerate random partitioning.
	‚Ä¢	Diversity among models improves ensemble performance.

Limitations
	‚Ä¢	Slightly higher bias than Random Forests.
	‚Ä¢	Less interpretable due to greater randomness.
	‚Ä¢	Random thresholds may underperform when precise split boundaries are crucial.

Despite these, Extra Trees often equal or surpass Random Forests in real-world tasks, especially with many noisy or irrelevant features.

‚∏ª

Key hyperparameters (conceptual view)
	‚Ä¢	n_estimators: number of trees in the ensemble.
	‚Ä¢	max_features: number of features considered for each split.
	‚Ä¢	max_depth, min_samples_split, min_samples_leaf: control complexity and prevent overfitting.
	‚Ä¢	bootstrap: whether to sample data with replacement (False by default).
	‚Ä¢	criterion: measure of split quality (e.g., Gini or entropy).

Increasing max_features raises correlation (lower diversity), while decreasing it enhances randomness but may increase bias.

‚∏ª

Evaluation focus

Evaluate Extra Trees similarly to Random Forests, emphasizing:
	‚Ä¢	Accuracy and F1-score for balanced datasets.
	‚Ä¢	ROC‚ÄìAUC and PR‚ÄìAUC for imbalanced problems.
	‚Ä¢	Stability across folds ‚Äî Extra Trees should show low variance in cross-validation results.
	‚Ä¢	Feature importance (permutation-based) to gauge interpretability.

Their performance tends to be more consistent across noisy datasets.

‚∏ª

When to use / When not to use

Use it when:
	‚Ä¢	The dataset has many noisy or irrelevant features.
	‚Ä¢	Speed and robustness are priorities.
	‚Ä¢	You want a simple ensemble with minimal tuning.

Avoid it when:
	‚Ä¢	Precise split optimization is critical (e.g., highly structured or rule-based data).
	‚Ä¢	Interpretability is a top concern.

‚∏ª

References

Canonical papers
	1.	Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely Randomized Trees. Machine Learning, 63(1), 3‚Äì42.
	2.	Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5‚Äì32.
	3.	Fern√°ndez-Delgado, M. et al. (2014). Do We Need Hundreds of Classifiers to Solve Real World Problems? Journal of Machine Learning Research, 15, 3133‚Äì3181.

Web resources
	‚Ä¢	Scikit-learn User Guide ‚Äî Extra Trees
https://scikit-learn.org/stable/modules/ensemble.html#extra-treesÔøº
	‚Ä¢	StatQuest ‚Äî Random Forests vs Extra Trees Explained
https://www.youtube.com/watch?v=sQ870aTKqiMÔøº


-----

Extra Trees demonstrated that more randomness can actually mean more generalization.
By skipping the search for the perfect split, they gained speed, simplicity, and diversity ‚Äî showing that perfection in individual trees is less important than harmony in the ensemble.

However, both Bagging and Random Forests share a common limitation: all their trees are built independently.
They do not learn from each other‚Äôs mistakes.
The next family of models ‚Äî Boosting algorithms ‚Äî transforms this independence into cooperation, where each new learner focuses on what the previous ones got wrong.

-----

#### AdaBoost.


-----


-----

#### Gradient Boosting (GBDT).



-----


-----

#### XGBoost.


-----


-----

#### LightGBM.




-----


-----


#### CatBoost.




-----


-----


### F. Neural Networks for Classification


#### MLP (Feed-Forward Neural Network)
	
	
#### CNN (Convolutional Neural Network) ‚Äì overview for image data.

#### RNN / LSTM / GRU ‚Äì overview for sequence data.

#### Transformer-based Classifier ‚Äì overview for text or sequential data.
	











-------------------------------------------------

2) Temas transversales (previos a los modelos)

Prop√≥sito: antes de estimar modelos, entender aspectos que cambian decisiones y m√©tricas.

Desbalanceo de clases

Estrategias: class_weight, re-muestreo (Under/Over), SMOTE/ADASYN, focal loss (para NN/boosting).

Cu√°ndo aplicarlas y riesgos (overfitting por oversampling, fuga de informaci√≥n en CV, etc.).

Calibraci√≥n de probabilidades

Platt scaling, Isot√≥nica, temperature scaling (NN). Evaluar con Brier y curvas de confiabilidad.

Selecci√≥n de umbral

Umbral fijo vs. dependiente de costos / prevalencia. Maximizar F1, J (Youden), utilidad esperada.

M√©tricas de evaluaci√≥n (resumen)

ROC‚ÄëAUC vs PR‚ÄëAUC (preferir PR‚ÄëAUC con clase rara), F1/FŒ≤, MCC, KS, Log‚Äëloss, Brier.

Curvas ROC/PR y calibraci√≥n; reporte por clase; matrices de costos cuando aplique.

Estos conceptos aparecer√°n expl√≠citamente en cada t√©cnica (secci√≥n ‚ÄúEvaluaci√≥n adecuada‚Äù y ‚ÄúBuenas pr√°cticas‚Äù).

3) Taxonom√≠a (qu√© cubriremos)

Lineales probabil√≠sticos

Regresi√≥n Log√≠stica (binaria y multiclase OvR/OvO)

LDA/QDA (An√°lisis Discriminante Lineal/Cuadr√°tico)

Naive Bayes (Gaussiano/Multinomial/Bernoulli)

M√°rgenes y planos

Perceptr√≥n

SVM (lineal y kernel: RBF, polinomial)

Basados en instancias

k‚ÄëNN

√Årboles

Decision Trees (CART, Gini/Entrop√≠a)

Ensamblajes

Bagging, Random Forest

AdaBoost, Gradient Boosting

XGBoost, LightGBM, CatBoost

Redes Neuronales (‚â• 3 t√©cnicas de clasificaci√≥n)

MLP (feed‚Äëforward) para tabular/texto vectorizado

CNN simple para im√°genes (clasificaci√≥n b√°sica)

RNN/LSTM/GRU para secuencias (p.ej., texto/tiempo) con salida de clase

4) Plantilla com√∫n por t√©cnica (doc)

Cada archivo en docs/ seguir√° esta estructura uniforme:

¬øQu√© es? (definici√≥n en 3‚Äì5 l√≠neas)

¬øPara qu√© sirve? (casos de uso t√≠picos)

Intuici√≥n (figura/visi√≥n geom√©trica o probabil√≠stica)

Fundamento matem√°tico (expresi√≥n/funci√≥n de p√©rdida & decisi√≥n)

Algoritmo de entrenamiento (pasos o pseudo‚Äëc√≥digo)

Supuestos & condiciones (cuando el modelo se comporta mejor)

Hiperpar√°metros clave (tabla con significado e impacto)

Complejidad (tiempo/espacio, n vs d)

Buenas pr√°cticas (escalado, regularizaci√≥n, validaci√≥n)

Pitfalls comunes (leakage, overfitting, multicolinealidad, etc.)

Implementaci√≥n en librer√≠as

Python: librer√≠a y clase/funci√≥n (sklearn.linear_model.LogisticRegression, xgboost.XGBClassifier, lightgbm.LGBMClassifier, catboost.CatBoostClassifier, torch.nn.Module/keras.Model, etc.) + par√°metros relevantes y su efecto.

R (opcional): glm, MASS::lda/qda, e1071::svm, randomForest, xgboost, lightgbm, catboost, keras.

C√≥digo m√≠nimo (Python y/o R, dataset sint√©tico + m√©trica principal)

Cu√°ndo S√ç usarla (id√≥nea) y cu√°ndo NO usarla (anti‚Äëpatrones)

Referencias

Papers (3): trabajos can√≥nicos/seminales.

Web (2): documentaci√≥n oficial o tutoriales reputados.

CASO DE LA REGRESI√ìN LOGISTICA


6) M√©tricas (resumen de docs/99-evaluation-metrics.md)

ROC‚ÄëAUC vs PR‚ÄëAUC (preferir PR‚ÄëAUC con clase positiva rara)

F1 / FŒ≤; MCC; Brier; KS; Log‚Äëloss

Curvas: ROC, PR, calibraci√≥n; umbral √≥ptimo por m√©trica/escenario de costos


